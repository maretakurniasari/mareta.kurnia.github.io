{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Selamat Datang Di Halaman Tugas Penambangan Data (Data Mining) \u00b6 Nama : Mareta Kurnia Sari NIM : 180411100090 Kelas : Penambangan Data 5D Jurusan : Teknik Informatika Angkatan : 2018 Dosen Pengampu : Mula'ab,S.Si.,M.Kom Alamat : Sumenep","title":"index"},{"location":"#selamat-datang-di-halaman-tugas-penambangan-data-data-mining","text":"Nama : Mareta Kurnia Sari NIM : 180411100090 Kelas : Penambangan Data 5D Jurusan : Teknik Informatika Angkatan : 2018 Dosen Pengampu : Mula'ab,S.Si.,M.Kom Alamat : Sumenep","title":"Selamat Datang Di Halaman Tugas Penambangan Data (Data Mining)"},{"location":"Mengukur Jarak/","text":"Mengukur Jarak Data \u00b6 Mengukur Jarak Tipe Data \u00b6 Pengertian : \u00b6 Tantangan dalam era dan zaman ini salah satunya datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn,v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yixi,yi disebut atribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya Minkowski Distance \u00b6 Kelompok Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan dimana m adalah bilangan riel positif dan xi dan yi adalah dua vektor dalam ruang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar. Manhattan Distance \u00b6 Manhattan distance adalah kasus khusus dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. Bila ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan Euclidean Distance \u00b6 Euclidean Distance adalah jarak yang paling terkenal yang digunakan untuk data numerik. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini. Average Distance \u00b6 Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adalah versi modikfikasid dari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan Weighted Euclidean Distance \u00b6 Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan dimana wi adalah bobot yang diberikan pada atribut ke i. Chord Distance \u00b6 Chord Distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan dimana \u2016 x \u20162 adalah L 2-norm Mahalanobis Distance \u00b6 Mahalanobis Distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antara data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan dimana S adalah matrik covariance data. Consine measure \u00b6 Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan dimana \u2225y\u22252 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn)y=(y1,y2,\u2026,yn) didefinisikan dengan Pearson Correlation \u00b6 Pearson Correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan The Pearson correlation kelemahannya adalah sensitif terhadap outlier Mengukur Jarak Atribut Binary \u00b6 Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d72 di mana qq adalah jumlah atribut yang sama dengan 1 untuk kedua objek ii dan jj, rr adalah jumlah atribut yang sama dengan 1 untuk objek ii tetapi 0 untuk objek jj, ss adalah jumlah atribut yang sama dengan 0 untuk objek ii tetapi 1 untuk objek jj, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek ii dan jj. Jumlah total atribut adalah pp, di mana p=q+r+s+tp=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antarii dan j adalah $$ d ( i , j ) = \\frac { r + s } { q + r + s + t } $$ Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya $$ d ( i , j ) = \\frac { r + s } { q + r + s } $$ Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek ii dan jj dapat dihitung dengan $$ \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) $$ Persamaan similarity ini disebut dengan Jaccard coefficient Mengukur Jarak Tipe categorical \u00b6 Overlay Metric \u00b6 Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) $$ dimana nn adalah banyaknya atribut, ai(x)ai(x) dan ai(y)ai(y) adalah nilai atribut ke ii yaitu AiAi dari masing masing objek xx dan yy, \u03b4 (ai(x),ai(y))\u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi. Value Difference Metric (VDM) \u00b6 VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | $$ dimana CCadalah banyaknya kelas, P(c|ai(x))P(c|ai(x)) adalah probabilitas bersyarat dimana kelas xx adalah cc dari atribut AiAi, yang memiliki nilai ai(x)ai(x), P(c|ai(y))P(c|ai(y)) adalah probabilitas bersyarat dimana kelas yy adalah cc dengan atribut AiAi memiliki nilai ai(y)ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| $$ diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) dan P(c|y) didekati dengan Naive Bayes, Minimum Risk Metric (MRM) \u00b6 Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$ Mengukur Jarak Tipe Ordinal \u00b6 Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal ff yang memiliki MfMf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. MM adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan ff adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: \u00b7 Nilai ff untuk objek ke-ii adalah xifxif, dan ff memiliki MfMf status urutan , mewakili peringkat 1,..,Mf1,..,Mf Ganti setiap xifxif dengan peringkatnya, rif\u2208{1...Mf}rif\u2208{1...Mf} \u00b7 Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rifrif dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ \u00b7 Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$ Menghitung Jarak Tipe Campuran \u00b6 Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar 0,0,1.0. Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan $$ d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } $$ dimana \u03b4fij=0\u03b4ijf=0 - jika xifxif atau xjfxjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek ii atau objek jj) \u00b7 jika xif=xjf=0xif=xjf=0 dan \u00b7 atribut ff adalah binary asymmetric, selain itu \u03b4fij=1\u03b4ijf=1 Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu.dfijdijf) dihitung bergantung pada tipenya, \u00b7 Jika ff adalah numerik, $$ d_{ij}^{f}=\\frac{ |x {if}-x {jf}|}{max_hx_{hf}-min_hx{hf}} $$ , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f \u00b7 Jika ff adalah nominal atau binary,$d_{ij}^{f}=0 $jika xif=xjfxif=xjf, sebaliknya dfij=1dijf=1 \u00b7 Jika ff adalah ordinal maka hitung rangking rifrif dan $$ \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} $$ , dan perlakukan zifzif sebagai numerik. Langkah - langkah Mengukur Jarak \u00b6 Alat dan Bahan \u00b6 library python yang digunakan adalah sebagai berikut : pandas, digunakan untuk data manajemen dan data analysis. scipy, merupakan library berisi kumpulan algoritma dan fungsi matematika. Langkah - langkah \u00b6 pertama : \u00b6 pertama - tama download datanya https://archive.ics.uci.edu/ml/machine-learning-n ndatabases/haberman/ Kedua : \u00b6 selanjutnya masukkan library yang telah disiapkan from scipy import stats import pandas as pd Ketiga : \u00b6 Selanjutnya memuat data csvnya df = pd . read_csv ( 'data jarak.csv' ) df setelah di run : 30 64 1 1.1 0 30 62 3 1 1 30 65 0 1 2 31 59 2 1 3 31 65 4 1 4 33 58 10 1 5 33 60 0 1 6 34 59 0 2 7 34 66 9 2 8 34 58 30 1 9 34 60 1 1 10 34 61 10 1 11 34 67 7 1 12 34 60 0 1 13 35 64 13 1 14 35 63 0 1 15 36 60 1 1 16 36 69 0 1 17 37 60 0 1 18 37 63 0 1 19 37 58 0 1 20 37 59 6 1 21 37 60 15 1 22 37 63 0 1 23 38 69 21 2 24 38 59 2 1 25 38 60 0 1 26 38 60 0 1 27 38 62 3 1 28 38 64 1 1 29 38 66 0 1 ... ... ... ... ... 275 67 66 0 1 276 67 61 0 1 277 67 65 0 1 278 68 67 0 1 279 68 68 0 1 280 69 67 8 2 281 69 60 0 1 282 69 65 0 1 283 69 66 0 1 284 70 58 0 2 285 70 58 4 2 286 70 66 14 1 287 70 67 0 1 288 70 68 0 1 289 70 59 8 1 290 70 63 0 1 291 71 68 2 1 292 72 63 0 2 293 72 58 0 1 294 72 64 0 1 295 72 67 3 1 296 73 62 0 1 297 73 68 0 1 298 74 65 3 2 299 74 63 0 1 300 75 62 1 1 301 76 67 0 1 302 77 65 3 1 303 78 65 1 2 304 83 58 2 2 Keempat : \u00b6 Kemudian membuat data penyimpanan / dictionary yang menampung nilai yang ditampilkan. selanjutnya mengambil data dari beberapa kolom pada csv dengan cara diiterasi serta, menghitungnya dengan berbagai metode yang telah disiapkan oleh pandas itu sendiri. kemudian hasil tersebut di disimpan pada penyimpanan yang tadi, dan memvisualisasikan hasil tersebut dalam bentuk data frame df = pd . read_csv ( 'data jarak.csv' ) k = df . iloc [ 10 : 16 ] k setelah di run: 30 64 1 1.1 10 34 61 10 1 11 34 67 7 1 12 34 60 0 1 13 35 64 13 1 14 35 63 0 1 15 36 60 1 1 kelima : \u00b6 numerical = [ 0 , 3 ] categorical = [ 1 , 2 , 6 , 7 ] binary = [ 4 , 5 , 8 ] ordinal = [ 1 , 2 ] from IPython.display import HTML , display import tabulate table = [ [ \"Data\" ] + [ \"Jarak\" ] + [ \"Numeric\" ] + [ \"Ordinal\" ] + [ \"Categorical\" ] + [ \"Binary\" ], [ \"v1-v2\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v1-v3\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v2-v3\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v3-v4\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v4-v5\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v5-v6\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) setelah di run: Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 0 0 0 0 v1-v3 0 0 0 0 0 v2-v3 0 0 0 0 0 v3-v4 0 0 0 0 0 v4-v5 0 0 0 0 0 v5-v6 0 0 0 0 0 Mengukur Jarak Numerik \u00b6 def chordDist ( v1 , v2 , jnis ): jmlh = 0 normv1 = 0 normv2 = 0 for x in range ( len ( jnis )): normv1 = normv1 + ( int ( k . values . tolist ()[ v1 ][ jnis [ x ]]) ** 2 ) normv2 = normv2 + ( int ( k . values . tolist ()[ v1 ][ jnis [ x ]]) ** 2 ) jmlh = jmlh + ( int ( k . values . tolist ()[ v1 ][ jnis [ x ]]) * int ( k . values . tolist ()[ v2 ][ jnis [ x ]])) return (( 2 - ( 2 * jmlh / ( normv1 * normv2 ))) ** 0.5 ) from IPython.display import HTML , display import tabulate table = [ [ \"Data\" ] + [ \"Jarak\" ] + [ \"Numeric\" ] + [ \"Ordinal\" ] + [ \"Categorical\" ] + [ \"Binary\" ], [ \"v1-v2\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 0 , 1 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v1-v3\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 0 , 2 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v2-v3\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 1 , 2 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v3-v4\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 2 , 3 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v4-v5\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 3 , 4 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v5-v6\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 2 , 3 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 1.41 0 0 0 v1-v3 0 1.41 0 0 0 v2-v3 0 1.41 0 0 0 v3-v4 0 1.41 0 0 0 v4-v5 0 1.41 0 0 0 v5-v6 0 1.41 0 0 0","title":"Mengukur Jarak Data"},{"location":"Mengukur Jarak/#mengukur-jarak-data","text":"","title":"Mengukur Jarak Data"},{"location":"Mengukur Jarak/#mengukur-jarak-tipe-data","text":"","title":"Mengukur Jarak Tipe Data"},{"location":"Mengukur Jarak/#pengertian","text":"Tantangan dalam era dan zaman ini salah satunya datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn,v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yixi,yi disebut atribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya","title":"Pengertian :"},{"location":"Mengukur Jarak/#minkowski-distance","text":"Kelompok Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan dimana m adalah bilangan riel positif dan xi dan yi adalah dua vektor dalam ruang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar.","title":"Minkowski Distance"},{"location":"Mengukur Jarak/#manhattan-distance","text":"Manhattan distance adalah kasus khusus dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. Bila ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan","title":"Manhattan Distance"},{"location":"Mengukur Jarak/#euclidean-distance","text":"Euclidean Distance adalah jarak yang paling terkenal yang digunakan untuk data numerik. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini.","title":"Euclidean Distance"},{"location":"Mengukur Jarak/#average-distance","text":"Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adalah versi modikfikasid dari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan","title":"Average Distance"},{"location":"Mengukur Jarak/#weighted-euclidean-distance","text":"Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan dimana wi adalah bobot yang diberikan pada atribut ke i.","title":"Weighted Euclidean Distance"},{"location":"Mengukur Jarak/#chord-distance","text":"Chord Distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan dimana \u2016 x \u20162 adalah L 2-norm","title":"Chord Distance"},{"location":"Mengukur Jarak/#mahalanobis-distance","text":"Mahalanobis Distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antara data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan dimana S adalah matrik covariance data.","title":"Mahalanobis Distance"},{"location":"Mengukur Jarak/#consine-measure","text":"Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan dimana \u2225y\u22252 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn)y=(y1,y2,\u2026,yn) didefinisikan dengan","title":"Consine measure"},{"location":"Mengukur Jarak/#pearson-correlation","text":"Pearson Correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan The Pearson correlation kelemahannya adalah sensitif terhadap outlier","title":"Pearson Correlation"},{"location":"Mengukur Jarak/#mengukur-jarak-atribut-binary","text":"Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d72 di mana qq adalah jumlah atribut yang sama dengan 1 untuk kedua objek ii dan jj, rr adalah jumlah atribut yang sama dengan 1 untuk objek ii tetapi 0 untuk objek jj, ss adalah jumlah atribut yang sama dengan 0 untuk objek ii tetapi 1 untuk objek jj, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek ii dan jj. Jumlah total atribut adalah pp, di mana p=q+r+s+tp=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antarii dan j adalah $$ d ( i , j ) = \\frac { r + s } { q + r + s + t } $$ Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya $$ d ( i , j ) = \\frac { r + s } { q + r + s } $$ Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek ii dan jj dapat dihitung dengan $$ \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) $$ Persamaan similarity ini disebut dengan Jaccard coefficient","title":"Mengukur Jarak Atribut Binary"},{"location":"Mengukur Jarak/#mengukur-jarak-tipe-categorical","text":"","title":"Mengukur Jarak Tipe categorical"},{"location":"Mengukur Jarak/#overlay-metric","text":"Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) $$ dimana nn adalah banyaknya atribut, ai(x)ai(x) dan ai(y)ai(y) adalah nilai atribut ke ii yaitu AiAi dari masing masing objek xx dan yy, \u03b4 (ai(x),ai(y))\u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi.","title":"Overlay Metric"},{"location":"Mengukur Jarak/#value-difference-metric-vdm","text":"VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | $$ dimana CCadalah banyaknya kelas, P(c|ai(x))P(c|ai(x)) adalah probabilitas bersyarat dimana kelas xx adalah cc dari atribut AiAi, yang memiliki nilai ai(x)ai(x), P(c|ai(y))P(c|ai(y)) adalah probabilitas bersyarat dimana kelas yy adalah cc dengan atribut AiAi memiliki nilai ai(y)ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| $$ diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) dan P(c|y) didekati dengan Naive Bayes,","title":"Value Difference Metric (VDM)"},{"location":"Mengukur Jarak/#minimum-risk-metric-mrm","text":"Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$","title":"Minimum Risk Metric (MRM)"},{"location":"Mengukur Jarak/#mengukur-jarak-tipe-ordinal","text":"Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal ff yang memiliki MfMf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. MM adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan ff adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: \u00b7 Nilai ff untuk objek ke-ii adalah xifxif, dan ff memiliki MfMf status urutan , mewakili peringkat 1,..,Mf1,..,Mf Ganti setiap xifxif dengan peringkatnya, rif\u2208{1...Mf}rif\u2208{1...Mf} \u00b7 Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rifrif dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ \u00b7 Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$","title":"Mengukur Jarak Tipe Ordinal"},{"location":"Mengukur Jarak/#menghitung-jarak-tipe-campuran","text":"Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar 0,0,1.0. Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan $$ d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } $$ dimana \u03b4fij=0\u03b4ijf=0 - jika xifxif atau xjfxjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek ii atau objek jj) \u00b7 jika xif=xjf=0xif=xjf=0 dan \u00b7 atribut ff adalah binary asymmetric, selain itu \u03b4fij=1\u03b4ijf=1 Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu.dfijdijf) dihitung bergantung pada tipenya, \u00b7 Jika ff adalah numerik, $$ d_{ij}^{f}=\\frac{ |x {if}-x {jf}|}{max_hx_{hf}-min_hx{hf}} $$ , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f \u00b7 Jika ff adalah nominal atau binary,$d_{ij}^{f}=0 $jika xif=xjfxif=xjf, sebaliknya dfij=1dijf=1 \u00b7 Jika ff adalah ordinal maka hitung rangking rifrif dan $$ \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} $$ , dan perlakukan zifzif sebagai numerik.","title":"Menghitung Jarak Tipe Campuran"},{"location":"Mengukur Jarak/#langkah-langkah-mengukur-jarak","text":"","title":"Langkah - langkah Mengukur Jarak"},{"location":"Mengukur Jarak/#alat-dan-bahan","text":"library python yang digunakan adalah sebagai berikut : pandas, digunakan untuk data manajemen dan data analysis. scipy, merupakan library berisi kumpulan algoritma dan fungsi matematika.","title":"Alat dan Bahan"},{"location":"Mengukur Jarak/#langkah-langkah","text":"","title":"Langkah - langkah"},{"location":"Mengukur Jarak/#pertama","text":"pertama - tama download datanya https://archive.ics.uci.edu/ml/machine-learning-n ndatabases/haberman/","title":"pertama :"},{"location":"Mengukur Jarak/#kedua","text":"selanjutnya masukkan library yang telah disiapkan from scipy import stats import pandas as pd","title":"Kedua :"},{"location":"Mengukur Jarak/#ketiga","text":"Selanjutnya memuat data csvnya df = pd . read_csv ( 'data jarak.csv' ) df setelah di run : 30 64 1 1.1 0 30 62 3 1 1 30 65 0 1 2 31 59 2 1 3 31 65 4 1 4 33 58 10 1 5 33 60 0 1 6 34 59 0 2 7 34 66 9 2 8 34 58 30 1 9 34 60 1 1 10 34 61 10 1 11 34 67 7 1 12 34 60 0 1 13 35 64 13 1 14 35 63 0 1 15 36 60 1 1 16 36 69 0 1 17 37 60 0 1 18 37 63 0 1 19 37 58 0 1 20 37 59 6 1 21 37 60 15 1 22 37 63 0 1 23 38 69 21 2 24 38 59 2 1 25 38 60 0 1 26 38 60 0 1 27 38 62 3 1 28 38 64 1 1 29 38 66 0 1 ... ... ... ... ... 275 67 66 0 1 276 67 61 0 1 277 67 65 0 1 278 68 67 0 1 279 68 68 0 1 280 69 67 8 2 281 69 60 0 1 282 69 65 0 1 283 69 66 0 1 284 70 58 0 2 285 70 58 4 2 286 70 66 14 1 287 70 67 0 1 288 70 68 0 1 289 70 59 8 1 290 70 63 0 1 291 71 68 2 1 292 72 63 0 2 293 72 58 0 1 294 72 64 0 1 295 72 67 3 1 296 73 62 0 1 297 73 68 0 1 298 74 65 3 2 299 74 63 0 1 300 75 62 1 1 301 76 67 0 1 302 77 65 3 1 303 78 65 1 2 304 83 58 2 2","title":"Ketiga :"},{"location":"Mengukur Jarak/#keempat","text":"Kemudian membuat data penyimpanan / dictionary yang menampung nilai yang ditampilkan. selanjutnya mengambil data dari beberapa kolom pada csv dengan cara diiterasi serta, menghitungnya dengan berbagai metode yang telah disiapkan oleh pandas itu sendiri. kemudian hasil tersebut di disimpan pada penyimpanan yang tadi, dan memvisualisasikan hasil tersebut dalam bentuk data frame df = pd . read_csv ( 'data jarak.csv' ) k = df . iloc [ 10 : 16 ] k setelah di run: 30 64 1 1.1 10 34 61 10 1 11 34 67 7 1 12 34 60 0 1 13 35 64 13 1 14 35 63 0 1 15 36 60 1 1","title":"Keempat :"},{"location":"Mengukur Jarak/#kelima","text":"numerical = [ 0 , 3 ] categorical = [ 1 , 2 , 6 , 7 ] binary = [ 4 , 5 , 8 ] ordinal = [ 1 , 2 ] from IPython.display import HTML , display import tabulate table = [ [ \"Data\" ] + [ \"Jarak\" ] + [ \"Numeric\" ] + [ \"Ordinal\" ] + [ \"Categorical\" ] + [ \"Binary\" ], [ \"v1-v2\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v1-v3\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v2-v3\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v3-v4\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v4-v5\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v5-v6\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) setelah di run: Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 0 0 0 0 v1-v3 0 0 0 0 0 v2-v3 0 0 0 0 0 v3-v4 0 0 0 0 0 v4-v5 0 0 0 0 0 v5-v6 0 0 0 0 0","title":"kelima :"},{"location":"Mengukur Jarak/#mengukur-jarak-numerik","text":"def chordDist ( v1 , v2 , jnis ): jmlh = 0 normv1 = 0 normv2 = 0 for x in range ( len ( jnis )): normv1 = normv1 + ( int ( k . values . tolist ()[ v1 ][ jnis [ x ]]) ** 2 ) normv2 = normv2 + ( int ( k . values . tolist ()[ v1 ][ jnis [ x ]]) ** 2 ) jmlh = jmlh + ( int ( k . values . tolist ()[ v1 ][ jnis [ x ]]) * int ( k . values . tolist ()[ v2 ][ jnis [ x ]])) return (( 2 - ( 2 * jmlh / ( normv1 * normv2 ))) ** 0.5 ) from IPython.display import HTML , display import tabulate table = [ [ \"Data\" ] + [ \"Jarak\" ] + [ \"Numeric\" ] + [ \"Ordinal\" ] + [ \"Categorical\" ] + [ \"Binary\" ], [ \"v1-v2\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 0 , 1 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v1-v3\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 0 , 2 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v2-v3\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 1 , 2 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v3-v4\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 2 , 3 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v4-v5\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 3 , 4 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v5-v6\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 2 , 3 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 1.41 0 0 0 v1-v3 0 1.41 0 0 0 v2-v3 0 1.41 0 0 0 v3-v4 0 1.41 0 0 0 v4-v5 0 1.41 0 0 0 v5-v6 0 1.41 0 0 0","title":"Mengukur Jarak Numerik"},{"location":"Missing Values/","text":"Missing Values using KNN \u00b6 KNN adalah algoritma yang berguna untuk mencocokkan suatu titik dengan tetangga terdekatnya dalam ruang multi-dimensi. Ini dapat digunakan untuk data yang kontinu, diskrit, ordinal, dan kategoris yang membuatnya sangat berguna untuk menangani semua jenis data yang hilang. Asumsi di balik menggunakan KNN untuk nilai yang hilang adalah bahwa nilai poin dapat didekati dengan nilai dari poin yang paling dekat dengannya, berdasarkan pada variabel lain. Mari kita simpan contoh sebelumnya dan tambahkan variabel lain, penghasilan orang tersebut. Sekarang kami memiliki tiga variabel, jenis kelamin, pendapatan dan tingkat depresi yang memiliki nilai yang hilang. Kami kemudian berasumsi bahwa orang-orang dengan pendapatan yang sama dan jenis kelamin yang sama cenderung memiliki tingkat depresi yang sama. Untuk nilai yang hilang, kita akan melihat jenis kelamin orang tersebut, pendapatannya, mencari k tetangga terdekatnya dan mendapatkan tingkat depresi mereka. Kita kemudian dapat memperkirakan tingkat depresi orang yang kita inginkan. Kalibrasi Parameter KNN \u00b6 Jumlah tetangga yang harus dicari \u00b6 Mengambil k rendah akan meningkatkan pengaruh kebisingan dan hasilnya akan kurang digeneralisasikan. Di sisi lain, mengambil k tinggi akan cenderung mengaburkan efek lokal yang persis apa yang kita cari. Juga disarankan untuk mengambil k yang aneh untuk kelas biner untuk menghindari ikatan. Metode agregasi untuk digunakan \u00b6 Di sini kita memungkinkan untuk mean aritmatika, median dan mode untuk variabel numerik dan mode untuk yang kategorikal Normalisasi data \u00b6 Ini adalah metode yang memungkinkan setiap atribut memberikan pengaruh yang sama dalam mengidentifikasi tetangga saat menghitung jenis jarak tertentu seperti yang Euclidean. Anda harus menormalkan data Anda ketika skala tidak memiliki arti dan / atau Anda memiliki skala tidak konsisten seperti sentimeter dan meter. Ini menyiratkan pengetahuan sebelumnya tentang data untuk mengetahui mana yang lebih penting. Algoritma secara otomatis menormalkan data ketika variabel numerik dan kategorikal disediakan. Atribut numerik jarak \u00b6 Di antara berbagai metrik jarak yang tersedia, kami akan fokus pada yang utama, Euclidean dan Manhattan. Euclidean adalah ukuran jarak yang baik untuk digunakan jika variabel input bertipe sama (mis. Semua lebar dan tinggi yang diukur). Jarak Manhattan adalah ukuran yang baik untuk digunakan jika variabel input tidak dalam jenis yang sama (seperti usia, tinggi, dll ...). Atribut kategorikal jarak \u00b6 tanpa transformasi sebelumnya, jarak yang berlaku terkait dengan frekuensi dan kesamaan. Atribut kategorikal hampir sama dengan nominal karena dengan tipe ini akan dinormalisasikan menjadi numerik atau angka untuk bisa dirukur jaraknya # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) setelah dirun: First Score Second Score Third Score 0 100.0 30.0 0.0 1 90.0 45.0 40.0 2 0.0 56.0 80.0 3 95.0 0.0 98.0 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Missing Values using KNN"},{"location":"Missing Values/#missing-values-using-knn","text":"KNN adalah algoritma yang berguna untuk mencocokkan suatu titik dengan tetangga terdekatnya dalam ruang multi-dimensi. Ini dapat digunakan untuk data yang kontinu, diskrit, ordinal, dan kategoris yang membuatnya sangat berguna untuk menangani semua jenis data yang hilang. Asumsi di balik menggunakan KNN untuk nilai yang hilang adalah bahwa nilai poin dapat didekati dengan nilai dari poin yang paling dekat dengannya, berdasarkan pada variabel lain. Mari kita simpan contoh sebelumnya dan tambahkan variabel lain, penghasilan orang tersebut. Sekarang kami memiliki tiga variabel, jenis kelamin, pendapatan dan tingkat depresi yang memiliki nilai yang hilang. Kami kemudian berasumsi bahwa orang-orang dengan pendapatan yang sama dan jenis kelamin yang sama cenderung memiliki tingkat depresi yang sama. Untuk nilai yang hilang, kita akan melihat jenis kelamin orang tersebut, pendapatannya, mencari k tetangga terdekatnya dan mendapatkan tingkat depresi mereka. Kita kemudian dapat memperkirakan tingkat depresi orang yang kita inginkan.","title":"Missing Values using KNN"},{"location":"Missing Values/#kalibrasi-parameter-knn","text":"","title":"Kalibrasi Parameter KNN"},{"location":"Missing Values/#jumlah-tetangga-yang-harus-dicari","text":"Mengambil k rendah akan meningkatkan pengaruh kebisingan dan hasilnya akan kurang digeneralisasikan. Di sisi lain, mengambil k tinggi akan cenderung mengaburkan efek lokal yang persis apa yang kita cari. Juga disarankan untuk mengambil k yang aneh untuk kelas biner untuk menghindari ikatan.","title":"Jumlah tetangga yang harus dicari"},{"location":"Missing Values/#metode-agregasi-untuk-digunakan","text":"Di sini kita memungkinkan untuk mean aritmatika, median dan mode untuk variabel numerik dan mode untuk yang kategorikal","title":"Metode agregasi untuk digunakan"},{"location":"Missing Values/#normalisasi-data","text":"Ini adalah metode yang memungkinkan setiap atribut memberikan pengaruh yang sama dalam mengidentifikasi tetangga saat menghitung jenis jarak tertentu seperti yang Euclidean. Anda harus menormalkan data Anda ketika skala tidak memiliki arti dan / atau Anda memiliki skala tidak konsisten seperti sentimeter dan meter. Ini menyiratkan pengetahuan sebelumnya tentang data untuk mengetahui mana yang lebih penting. Algoritma secara otomatis menormalkan data ketika variabel numerik dan kategorikal disediakan.","title":"Normalisasi data"},{"location":"Missing Values/#atribut-numerik-jarak","text":"Di antara berbagai metrik jarak yang tersedia, kami akan fokus pada yang utama, Euclidean dan Manhattan. Euclidean adalah ukuran jarak yang baik untuk digunakan jika variabel input bertipe sama (mis. Semua lebar dan tinggi yang diukur). Jarak Manhattan adalah ukuran yang baik untuk digunakan jika variabel input tidak dalam jenis yang sama (seperti usia, tinggi, dll ...).","title":"Atribut numerik jarak"},{"location":"Missing Values/#atribut-kategorikal-jarak","text":"tanpa transformasi sebelumnya, jarak yang berlaku terkait dengan frekuensi dan kesamaan. Atribut kategorikal hampir sama dengan nominal karena dengan tipe ini akan dinormalisasikan menjadi numerik atau angka untuk bisa dirukur jaraknya # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) setelah dirun: First Score Second Score Third Score 0 100.0 30.0 0.0 1 90.0 45.0 40.0 2 0.0 56.0 80.0 3 95.0 0.0 98.0 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Atribut kategorikal jarak"},{"location":"Statistika Deskriptif/","text":"Statistika Deskriptif \u00b6 Pengertian : \u00b6 Statistika Deskriptif adalah metode representasi keseluruhan himpunan data spesifik dengan memberikan ringkasan pendek tentang sampel dan ukuran data Statistika Deskriptif juga merupakan metode yang sangat sederhana karena hanya mendeskripsikan kondisi dari data yang dimiliki dan menyajikannya dalam bentuk tabel diagram grafik dan bentuk lainnya yang di tampilkan dalam uraian singkat dan terbatas, sehingga dapat memberikan informasi yang berguna. Tipe Statistik Deskriptif : \u00b6 Mean \u00b6 Mean merupakan rata-rata dari keseluruhan angka. Mean didapatkan dari hasil penjumlahan dari keseluruhan angka yang dibagi dengan banyaknya angka itu sendiri. Untuk menghitung data, kita misalkan N data dengan rumus berikut : $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$ Keterangan : x bar = x rata-rata = nilai rata-rata sampel x = data ke n n = banyak data Median \u00b6 Median merupakan nilai tengah dari sebuah urutan data, median disimbolkan dengan Me. Nilai dari median akan sama dengan nilai Quartil 2 / Q2. dalam mencari median yang banyak n dari data ganjil dan genap memiliki cara perhitungan data yang berbeda, dengan rumus sebagai berikut : $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ Keterangan : Me = Nilai tengah dari kelompok data n = banyak data Modus \u00b6 Modus merupakan nilai / angka yang paling sering ditemukan dalam sebuah kelompok angka, atau data yang paling sering muncul, atau memiliki frekuensi tertinggi. Modus dilambangkan dengan Mo. dapat dihitung denganrumus berikut : $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ keterangan : Mo = modus dari kelompok data Tb = tepi bawah dari elemen modus b1 = selisih frekuensi antara elemen modus dengan elemet sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen sesudahnya p = panjang interval nilai b1 dan b2 \u2013> adalah mutlak / selalu positif Varians \u00b6 Varians adalah ukuran penyebaran setiap nilai dalam suatu himpunan data dari rata-rata. Dalam proses mencari varian terdapat langkah yang harus dilakukan, dengan mengambil ukuran jarak dari setiap nilai dan mengurangi rata-rata dari setiap nilai dalam data, kemudian hasil dari ukuran jarak tersebut dikuadratkan dan membagi jumlah kuadrat dengan jumlah nilai dalam himpunan data. Dapat dihitung dengan rumus berikut : $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ Keterangan : Xi = titik data x bar = rata-rata dari semua titik data n = banyak dari dari anggota data Standar Deviasi \u00b6 Standar Deviasi merupakan simpanan baku atau ukuran dispersi kumpulan data relatif terhadap rata-rata atau lebih simpelnya adalah akar kuadrat positif dari varian. Standar deviasi dihitung dengan mengakar kuadratkan nilai dari varians. jika titik data lebih dari rata-rata dalam kumpulan data maka semakin tinggi standar deviasi. dapat dihitung dengan rumus berikut : $$ \\sigma^ = \\sqrt {{\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n}} $$ Keterangan : xi = nilai x ke i x = rata rata n = ukuran sampel Skewness \u00b6 Skewness merupakan kemiringan atau ketidak simetrisan pada suatu distribusi statistik dimana kurva tampak condong ke kiri / ke kanan. Skewness digunakan untuk menentukan sejauh mana perbedaan suatu distribusi dengan distribusi normal. Dalam distribusi normal grafik muncul seperti kurva berbentuk lonceng. ketika suatu distribusi mengalami kemiringan ke sebelah kanan dan ekor di sisi kanan kurva lebih panjang dari ekor sisi kiri kurva maka situasi ini dikatakan kemiringan positif dan sebaliknya dikatakan kemiringan negative. Skewness dapat dihitung menggunakan rumus sebagai berikut: $$ Skewness = {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$ Keterangan : xi= titik data x bar = rata-rata dari distribusi n = jumlah titik dalam distribusi o = standar deviasi Quartile Quartile merupakan nilai-nilai yang membagi data yang telah diurutkan kedalam 4 bagian yang sama besar. Kuartil dinotasikan dengan notasi Q. Kuartil terdiri dari 3, yaitu kuartil pertama Q1, kuartil kedua Q2, dan kuartil ketiga Q3. Untuk menentukan kuartil pada data tunggal, kita harus mempertimbangkan banyaknya data n terlebih dahulu. Penghitungan quartil tergantung dari kondisi banyaknya data tersebut, Dalam mencari quatile kita dapat menggunakan rumus berikut ini: $$ Q_1 = (n + 1) {1\\over 4} $$ $$ Q_2 = (n + 1) {1\\over 2} $$ $$ Q_3 = (n + 1) {3\\over 4} $$ Keterangan : Q = nilai quartil n = banyak data Penerapan Statistik Deskriptif Menggunakan Python \u00b6 Alat dan Bahan : \u00b6 Pada penerapan ini saya menggunakan 100 data random yang disimpan dalam bentuk .csv dan untuk mempermudah dalam penerapan tersebut, perlu disiapkan library python yang dapat didownload secara gratis. library python yang digunakan adalah sebagai berikut : pandas, digunakan untuk data manajemen dan data analysis. scipy, merupakan library berisi kumpulan algoritma dan fungsi matematika. Langkah - Langkah \u00b6 Pertama \u00b6 Pertama-tama masukkan library yang sudah disiapkan from scipy import stats import pandas as pd Kedua \u00b6 Selanjutnya memuat data csv yang telah disiapkan df = pd . read_csv ( 'data.csv' ) df setelah di run maka program akan menampilkan seperti berikut : tinggi badan berat badan tekanan darah lingkar perut 0 165 53 112 50 1 152 66 104 69 2 171 55 120 42 3 155 59 127 63 4 158 49 111 47 5 173 51 103 51 6 165 71 119 47 7 157 46 123 43 8 169 80 101 62 9 160 48 130 41 10 157 62 127 60 11 157 60 106 47 12 151 67 129 70 13 177 43 111 48 14 159 52 119 43 15 179 64 117 47 16 163 76 106 48 17 164 57 113 48 18 155 61 121 41 19 174 67 124 66 20 154 52 122 61 21 155 45 117 58 22 165 49 108 51 23 168 72 127 57 24 165 77 116 56 25 161 49 113 43 26 156 64 127 57 27 166 49 125 68 28 164 78 129 63 29 151 46 100 57 ... ... ... ... ... 70 170 73 107 42 71 157 70 105 63 72 180 44 100 54 73 162 49 116 62 74 171 62 103 41 75 169 42 126 59 76 165 51 111 66 77 157 61 109 56 78 163 79 101 65 79 151 58 119 60 80 161 54 129 40 81 177 72 106 63 82 179 46 117 40 83 162 78 106 41 84 172 69 116 60 85 167 67 115 44 86 162 54 117 55 87 170 72 128 59 88 159 78 124 44 89 178 43 105 48 90 177 79 111 67 91 165 62 106 70 92 170 56 119 66 93 150 59 117 63 94 163 65 115 58 95 164 76 129 45 96 160 55 129 49 97 157 40 124 49 98 156 47 123 61 99 162 65 126 67 100 rows \u00d7 4 columns Ketiga \u00b6 Kemudian membuat data penyimpanan / dictionary yang menampung nilai yang ditampilkan. selanjutnya mengambil data dari beberapa kolom pada csv dengan cara diiterasi serta, menghitungnya dengan berbagai metode yang telah disiapkan oleh pandas itu sendiri. kemudian hasil tersebut di disimpan pada penyimpanan yang tadi. from IPython.display import HTML , display import tabulate table = [ [ \"method\" ] + [ x for x in df . columns ], [ \"count()\" ] + [ df [ col ] . count () for col in df . columns ], [ \"mean()\" ] + [ df [ col ] . mean () for col in df . columns ], [ \"std()\" ] + [ \"{:.2f}\" . format ( df [ col ] . std ()) for col in df . columns ], [ \"min()\" ] + [ df [ col ] . min () for col in df . columns ], [ \"max()\" ] + [ df [ col ] . max () for col in df . columns ], [ \"q1()\" ] + [ df [ col ] . quantile ( 0.25 ) for col in df . columns ], [ \"q2()\" ] + [ df [ col ] . quantile ( 0.50 ) for col in df . columns ], [ \"q3\" ] + [ df [ col ] . quantile ( 0.75 ) for col in df . columns ], [ \"skew\" ] + [ \"{:.2f}\" . format ( df [ col ] . skew ()) for col in df . columns ], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) Keempat \u00b6 Yang terakhir yaitu memvisualisasikan hasil tersebut dalam bentuk data frame display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) setelah di run maka program akan menampilkan seperti berikut : method tinggi badan berat badan tekanan darah lingkar perut count() 100 100 100 100 mean() 164.8 60.43 114.88 55.51 std() 7.86 11.81 8.81 9.59 min() 150 40 100 40 max() 180 80 130 70 q1() 158.0 49.75 106.75 47.0 q2() 165.0 60.5 115.5 57.0 q3 171.0 71.0 122.25 63.25 skew 0.07 0.06 -0.04 -0.15 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"statistika deskriptif"},{"location":"Statistika Deskriptif/#statistika-deskriptif","text":"","title":"Statistika Deskriptif"},{"location":"Statistika Deskriptif/#pengertian","text":"Statistika Deskriptif adalah metode representasi keseluruhan himpunan data spesifik dengan memberikan ringkasan pendek tentang sampel dan ukuran data Statistika Deskriptif juga merupakan metode yang sangat sederhana karena hanya mendeskripsikan kondisi dari data yang dimiliki dan menyajikannya dalam bentuk tabel diagram grafik dan bentuk lainnya yang di tampilkan dalam uraian singkat dan terbatas, sehingga dapat memberikan informasi yang berguna.","title":"Pengertian :"},{"location":"Statistika Deskriptif/#tipe-statistik-deskriptif","text":"","title":"Tipe Statistik Deskriptif :"},{"location":"Statistika Deskriptif/#mean","text":"Mean merupakan rata-rata dari keseluruhan angka. Mean didapatkan dari hasil penjumlahan dari keseluruhan angka yang dibagi dengan banyaknya angka itu sendiri. Untuk menghitung data, kita misalkan N data dengan rumus berikut : $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$ Keterangan : x bar = x rata-rata = nilai rata-rata sampel x = data ke n n = banyak data","title":"Mean"},{"location":"Statistika Deskriptif/#median","text":"Median merupakan nilai tengah dari sebuah urutan data, median disimbolkan dengan Me. Nilai dari median akan sama dengan nilai Quartil 2 / Q2. dalam mencari median yang banyak n dari data ganjil dan genap memiliki cara perhitungan data yang berbeda, dengan rumus sebagai berikut : $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ Keterangan : Me = Nilai tengah dari kelompok data n = banyak data","title":"Median"},{"location":"Statistika Deskriptif/#modus","text":"Modus merupakan nilai / angka yang paling sering ditemukan dalam sebuah kelompok angka, atau data yang paling sering muncul, atau memiliki frekuensi tertinggi. Modus dilambangkan dengan Mo. dapat dihitung denganrumus berikut : $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ keterangan : Mo = modus dari kelompok data Tb = tepi bawah dari elemen modus b1 = selisih frekuensi antara elemen modus dengan elemet sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen sesudahnya p = panjang interval nilai b1 dan b2 \u2013> adalah mutlak / selalu positif","title":"Modus"},{"location":"Statistika Deskriptif/#varians","text":"Varians adalah ukuran penyebaran setiap nilai dalam suatu himpunan data dari rata-rata. Dalam proses mencari varian terdapat langkah yang harus dilakukan, dengan mengambil ukuran jarak dari setiap nilai dan mengurangi rata-rata dari setiap nilai dalam data, kemudian hasil dari ukuran jarak tersebut dikuadratkan dan membagi jumlah kuadrat dengan jumlah nilai dalam himpunan data. Dapat dihitung dengan rumus berikut : $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ Keterangan : Xi = titik data x bar = rata-rata dari semua titik data n = banyak dari dari anggota data","title":"Varians"},{"location":"Statistika Deskriptif/#standar-deviasi","text":"Standar Deviasi merupakan simpanan baku atau ukuran dispersi kumpulan data relatif terhadap rata-rata atau lebih simpelnya adalah akar kuadrat positif dari varian. Standar deviasi dihitung dengan mengakar kuadratkan nilai dari varians. jika titik data lebih dari rata-rata dalam kumpulan data maka semakin tinggi standar deviasi. dapat dihitung dengan rumus berikut : $$ \\sigma^ = \\sqrt {{\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n}} $$ Keterangan : xi = nilai x ke i x = rata rata n = ukuran sampel","title":"Standar Deviasi"},{"location":"Statistika Deskriptif/#skewness","text":"Skewness merupakan kemiringan atau ketidak simetrisan pada suatu distribusi statistik dimana kurva tampak condong ke kiri / ke kanan. Skewness digunakan untuk menentukan sejauh mana perbedaan suatu distribusi dengan distribusi normal. Dalam distribusi normal grafik muncul seperti kurva berbentuk lonceng. ketika suatu distribusi mengalami kemiringan ke sebelah kanan dan ekor di sisi kanan kurva lebih panjang dari ekor sisi kiri kurva maka situasi ini dikatakan kemiringan positif dan sebaliknya dikatakan kemiringan negative. Skewness dapat dihitung menggunakan rumus sebagai berikut: $$ Skewness = {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$ Keterangan : xi= titik data x bar = rata-rata dari distribusi n = jumlah titik dalam distribusi o = standar deviasi Quartile Quartile merupakan nilai-nilai yang membagi data yang telah diurutkan kedalam 4 bagian yang sama besar. Kuartil dinotasikan dengan notasi Q. Kuartil terdiri dari 3, yaitu kuartil pertama Q1, kuartil kedua Q2, dan kuartil ketiga Q3. Untuk menentukan kuartil pada data tunggal, kita harus mempertimbangkan banyaknya data n terlebih dahulu. Penghitungan quartil tergantung dari kondisi banyaknya data tersebut, Dalam mencari quatile kita dapat menggunakan rumus berikut ini: $$ Q_1 = (n + 1) {1\\over 4} $$ $$ Q_2 = (n + 1) {1\\over 2} $$ $$ Q_3 = (n + 1) {3\\over 4} $$ Keterangan : Q = nilai quartil n = banyak data","title":"Skewness"},{"location":"Statistika Deskriptif/#penerapan-statistik-deskriptif-menggunakan-python","text":"","title":"Penerapan Statistik Deskriptif Menggunakan Python"},{"location":"Statistika Deskriptif/#alat-dan-bahan","text":"Pada penerapan ini saya menggunakan 100 data random yang disimpan dalam bentuk .csv dan untuk mempermudah dalam penerapan tersebut, perlu disiapkan library python yang dapat didownload secara gratis. library python yang digunakan adalah sebagai berikut : pandas, digunakan untuk data manajemen dan data analysis. scipy, merupakan library berisi kumpulan algoritma dan fungsi matematika.","title":"Alat dan Bahan :"},{"location":"Statistika Deskriptif/#langkah-langkah","text":"","title":"Langkah - Langkah"},{"location":"Statistika Deskriptif/#pertama","text":"Pertama-tama masukkan library yang sudah disiapkan from scipy import stats import pandas as pd","title":"Pertama"},{"location":"Statistika Deskriptif/#kedua","text":"Selanjutnya memuat data csv yang telah disiapkan df = pd . read_csv ( 'data.csv' ) df setelah di run maka program akan menampilkan seperti berikut : tinggi badan berat badan tekanan darah lingkar perut 0 165 53 112 50 1 152 66 104 69 2 171 55 120 42 3 155 59 127 63 4 158 49 111 47 5 173 51 103 51 6 165 71 119 47 7 157 46 123 43 8 169 80 101 62 9 160 48 130 41 10 157 62 127 60 11 157 60 106 47 12 151 67 129 70 13 177 43 111 48 14 159 52 119 43 15 179 64 117 47 16 163 76 106 48 17 164 57 113 48 18 155 61 121 41 19 174 67 124 66 20 154 52 122 61 21 155 45 117 58 22 165 49 108 51 23 168 72 127 57 24 165 77 116 56 25 161 49 113 43 26 156 64 127 57 27 166 49 125 68 28 164 78 129 63 29 151 46 100 57 ... ... ... ... ... 70 170 73 107 42 71 157 70 105 63 72 180 44 100 54 73 162 49 116 62 74 171 62 103 41 75 169 42 126 59 76 165 51 111 66 77 157 61 109 56 78 163 79 101 65 79 151 58 119 60 80 161 54 129 40 81 177 72 106 63 82 179 46 117 40 83 162 78 106 41 84 172 69 116 60 85 167 67 115 44 86 162 54 117 55 87 170 72 128 59 88 159 78 124 44 89 178 43 105 48 90 177 79 111 67 91 165 62 106 70 92 170 56 119 66 93 150 59 117 63 94 163 65 115 58 95 164 76 129 45 96 160 55 129 49 97 157 40 124 49 98 156 47 123 61 99 162 65 126 67 100 rows \u00d7 4 columns","title":"Kedua"},{"location":"Statistika Deskriptif/#ketiga","text":"Kemudian membuat data penyimpanan / dictionary yang menampung nilai yang ditampilkan. selanjutnya mengambil data dari beberapa kolom pada csv dengan cara diiterasi serta, menghitungnya dengan berbagai metode yang telah disiapkan oleh pandas itu sendiri. kemudian hasil tersebut di disimpan pada penyimpanan yang tadi. from IPython.display import HTML , display import tabulate table = [ [ \"method\" ] + [ x for x in df . columns ], [ \"count()\" ] + [ df [ col ] . count () for col in df . columns ], [ \"mean()\" ] + [ df [ col ] . mean () for col in df . columns ], [ \"std()\" ] + [ \"{:.2f}\" . format ( df [ col ] . std ()) for col in df . columns ], [ \"min()\" ] + [ df [ col ] . min () for col in df . columns ], [ \"max()\" ] + [ df [ col ] . max () for col in df . columns ], [ \"q1()\" ] + [ df [ col ] . quantile ( 0.25 ) for col in df . columns ], [ \"q2()\" ] + [ df [ col ] . quantile ( 0.50 ) for col in df . columns ], [ \"q3\" ] + [ df [ col ] . quantile ( 0.75 ) for col in df . columns ], [ \"skew\" ] + [ \"{:.2f}\" . format ( df [ col ] . skew ()) for col in df . columns ], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' )))","title":"Ketiga"},{"location":"Statistika Deskriptif/#keempat","text":"Yang terakhir yaitu memvisualisasikan hasil tersebut dalam bentuk data frame display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) setelah di run maka program akan menampilkan seperti berikut : method tinggi badan berat badan tekanan darah lingkar perut count() 100 100 100 100 mean() 164.8 60.43 114.88 55.51 std() 7.86 11.81 8.81 9.59 min() 150 40 100 40 max() 180 80 130 70 q1() 158.0 49.75 106.75 47.0 q2() 165.0 60.5 115.5 57.0 q3 171.0 71.0 122.25 63.25 skew 0.07 0.06 -0.04 -0.15 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Keempat"},{"location":"decision3/","text":"DECISION TREE \u00b6 \u200b Decision tree merupakan metode klarifikasi yang sering digunakan atau metode paling polpuler ,keunggulannya adalah mudah di interprestasi oleh manusia .dicision tree merupakan suatu prediksi yang berupa pohon atau bisa disebut stuktur beriharki,konsep decision tree adalah mengubah data yang ada menjadi pohon keputusan dan aturan aturan keputusan. CARA MEMBUAT DECISION TREE \u00b6 \u200b Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : Entropy \u00b6 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S Gain \u00b6 kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen (kolom, data): kelas=[] for i in range (len(data)): if data.values.tolist()[i][kolom] not in kelas: kelas.append(data.values.tolist()[i][kolom]) return kelas kelas=banyak_elemen(df.shape[1]-1, df) outlook=banyak_elemen(df.shape[1]-5,df) temp=banyak_elemen(df.shape[1]-4,df) humidity=banyak_elemen(df.shape[1]-3,df) windy=banyak_elemen(df.shape[1]-2,df) print(kelas,outlook,temp,humidity,windy)` ['no', 'yes'] ['sunny', 'overcast', 'rainy'] ['hot', 'mild', 'cool'] ['high', 'normal'] [False, True] Fungsi countvKelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas(kelas,kolomKelas,data): hasil=[] for x in range(len(kelas)): hasil.append(0) for i in range (len(data)): for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil pKelas=countvKelas(kelas,df.shape[1]-1,df) pKelas [5, 9] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy(T): hasil=0 jumlah=0 for y in T: jumlah+=y for z in range (len(T)): if jumlah!=0: T[z]=T[z]/jumlah for i in T: if i != 0: hasil-=i*math.log(i,2) return hasil def e_list(atribut,n): temp=[] tx=t_list(atribut,n) for i in range (len(atribut)): ent=entropy(tx[i]) temp.append(ent) return temp tOutlook=t_list(outlook,5) tTemp=t_list(temp,4) tHum=t_list(humidity,3) tWin=t_list(windy,2) print(\"Sunny, Overcast, Rainy\",eOutlook) print(\"Hot, Mild, Cold\", eTemp) print(\"High, Normal\", eHum) print(\"False, True\", eWin) Sunny, Overcast, Rainy [0.9709505944546686, 0.0, 0.9709505944546686] Hot, Mild, Cold [1.0, 0.9182958340544896, 0.8112781244591328] High, Normal [0.9852281360342516, 0.5916727785823275] False, True [0.8112781244591328, 1.0] Contoh \u00b6 berikut contoh data yang akan di rubah menjadi decision tree < table border = \"1\" class = \"dataframe\" > < thead > 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari entropy(s) dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Decision Tree"},{"location":"decision3/#decision-tree","text":"\u200b Decision tree merupakan metode klarifikasi yang sering digunakan atau metode paling polpuler ,keunggulannya adalah mudah di interprestasi oleh manusia .dicision tree merupakan suatu prediksi yang berupa pohon atau bisa disebut stuktur beriharki,konsep decision tree adalah mengubah data yang ada menjadi pohon keputusan dan aturan aturan keputusan.","title":"DECISION TREE"},{"location":"decision3/#cara-membuat-decision-tree","text":"\u200b Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain :","title":"CARA MEMBUAT DECISION TREE"},{"location":"decision3/#entropy","text":"$$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S","title":"Entropy"},{"location":"decision3/#gain","text":"kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen (kolom, data): kelas=[] for i in range (len(data)): if data.values.tolist()[i][kolom] not in kelas: kelas.append(data.values.tolist()[i][kolom]) return kelas kelas=banyak_elemen(df.shape[1]-1, df) outlook=banyak_elemen(df.shape[1]-5,df) temp=banyak_elemen(df.shape[1]-4,df) humidity=banyak_elemen(df.shape[1]-3,df) windy=banyak_elemen(df.shape[1]-2,df) print(kelas,outlook,temp,humidity,windy)` ['no', 'yes'] ['sunny', 'overcast', 'rainy'] ['hot', 'mild', 'cool'] ['high', 'normal'] [False, True] Fungsi countvKelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas(kelas,kolomKelas,data): hasil=[] for x in range(len(kelas)): hasil.append(0) for i in range (len(data)): for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil pKelas=countvKelas(kelas,df.shape[1]-1,df) pKelas [5, 9] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy(T): hasil=0 jumlah=0 for y in T: jumlah+=y for z in range (len(T)): if jumlah!=0: T[z]=T[z]/jumlah for i in T: if i != 0: hasil-=i*math.log(i,2) return hasil def e_list(atribut,n): temp=[] tx=t_list(atribut,n) for i in range (len(atribut)): ent=entropy(tx[i]) temp.append(ent) return temp tOutlook=t_list(outlook,5) tTemp=t_list(temp,4) tHum=t_list(humidity,3) tWin=t_list(windy,2) print(\"Sunny, Overcast, Rainy\",eOutlook) print(\"Hot, Mild, Cold\", eTemp) print(\"High, Normal\", eHum) print(\"False, True\", eWin) Sunny, Overcast, Rainy [0.9709505944546686, 0.0, 0.9709505944546686] Hot, Mild, Cold [1.0, 0.9182958340544896, 0.8112781244591328] High, Normal [0.9852281360342516, 0.5916727785823275] False, True [0.8112781244591328, 1.0]","title":"Gain"},{"location":"decision3/#contoh","text":"berikut contoh data yang akan di rubah menjadi decision tree < table border = \"1\" class = \"dataframe\" > < thead > 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari entropy(s) dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Contoh"}]}