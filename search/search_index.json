{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Selamat Datang Di Halaman Tugas Penambangan Data (Data Mining) \u00b6 \u00b6 Nama : Mareta Kurnia Sari NIM : 180411100090 Kelas : Penambangan Data 5D Jurusan : Teknik Informatika Angkatan : 2018 Dosen Pengampu : Mula'ab,S.Si.,M.Kom Alamat : Sumenep","title":"Index"},{"location":"#selamat-datang-di-halaman-tugas-penambangan-data-data-mining","text":"Nama : Mareta Kurnia Sari NIM : 180411100090 Kelas : Penambangan Data 5D Jurusan : Teknik Informatika Angkatan : 2018 Dosen Pengampu : Mula'ab,S.Si.,M.Kom Alamat : Sumenep","title":"Selamat Datang Di Halaman Tugas Penambangan Data (Data Mining)\u00b6"},{"location":"DECISION TREE/","text":"DECISION TREE \u00b6 \u00b6 Decision tree merupakan metode klarifikasi yang sering digunakan atau metode paling polpuler ,keunggulannya adalah mudah di interprestasi oleh manusia .dicision tree merupakan suatu prediksi yang berupa pohon atau bisa disebut stuktur beriharki,konsep decision tree adalah mengubah data yang ada menjadi pohon keputusan dan aturan aturan keputusan. CARA MEMBUAT DECISION TREE \u00b6 \u00b6 Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : Entropy \u00b6 \u00b6 Entropy(S)=n\u2211i=1\u2212pilog2piEntropy(S)=\u2211i=1n\u2212pilog2pi keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S Gain \u00b6 \u00b6 kemudian hitung nilai gain menggunakan rumus :GAIN(S,A)=entropy(S)\u2212n\u2211i=1|Si||s|\u2217entropy(Si)GAIN(S,A)=entropy(S)\u2212\u2211i=1n|Si||s|\u2217entropy(Si)keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen (kolom, data): kelas=[] for i in range (len(data)): if data.values.tolist()[i][kolom] not in kelas: kelas.append(data.values.tolist()[i][kolom]) return kelas kelas=banyak_elemen(df.shape[1]-1, df) outlook=banyak_elemen(df.shape[1]-5,df) temp=banyak_elemen(df.shape[1]-4,df) humidity=banyak_elemen(df.shape[1]-3,df) windy=banyak_elemen(df.shape[1]-2,df) print(kelas,outlook,temp,humidity,windy)` ['no', 'yes'] ['sunny', 'overcast', 'rainy'] ['hot', 'mild', 'cool'] ['high', 'normal'] [False, True] Fungsi countvKelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas(kelas,kolomKelas,data): hasil=[] for x in range(len(kelas)): hasil.append(0) for i in range (len(data)): for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil pKelas=countvKelas(kelas,df.shape[1]-1,df) pKelas [5, 9] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy(T): hasil=0 jumlah=0 for y in T: jumlah+=y for z in range (len(T)): if jumlah!=0: T[z]=T[z]/jumlah for i in T: if i != 0: hasil-=i*math.log(i,2) return hasil def e_list(atribut,n): temp=[] tx=t_list(atribut,n) for i in range (len(atribut)): ent=entropy(tx[i]) temp.append(ent) return temp tOutlook=t_list(outlook,5) tTemp=t_list(temp,4) tHum=t_list(humidity,3) tWin=t_list(windy,2) print(\"Sunny, Overcast, Rainy\",eOutlook) print(\"Hot, Mild, Cold\", eTemp) print(\"High, Normal\", eHum) print(\"False, True\", eWin) Sunny, Overcast, Rainy [0.9709505944546686, 0.0, 0.9709505944546686] Hot, Mild, Cold [1.0, 0.9182958340544896, 0.8112781244591328] High, Normal [0.9852281360342516, 0.5916727785823275] False, True [0.8112781244591328, 1.0] Contoh \u00b6 \u00b6 berikut contoh data yang akan di rubah menjadi decision tree <table border=\"1\" class=\"dataframe\"> <thead> 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari entropy(s) dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20Entropy(S)=n\u2211i=1\u2212pilog2piEntropy(S)=\u2211i=1n\u2212pilog2pi Entropy(S)=\u221210/20\u2217log210/20\u221210/20\u2217log210/20Entropy(S)=\u221210/20\u2217log210/20\u221210/20\u2217log210/20 Entropy(S)=1Entropy(S)=1 lalu kita menghitu gain setiap kolom di atas:GAIN(GENDER)=entropy(S)\u2212n\u2211i=1|Si||s|\u2217entropy(Si)GAIN(GENDER)=entropy(S)\u2212\u2211i=1n|Si||s|\u2217entropy(Si) GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] \u200b = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) \u200b =1-(10/20 x 0,970951)+(10/20 x 0,970951) \u200b =1-(0,4485475+0,4485475) \u200b =1-0,970951 \u200b =0.029049 GAIN(CARTIPE)=entropy(S)\u2212n\u2211i=1|Si||s|\u2217entropy(Si)GAIN(CARTIPE)=entropy(S)\u2212\u2211i=1n|Si||s|\u2217entropy(Si) GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] \u200b = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) \u200b =1-(0,162256+0+0,217426) \u200b =1-0,379681 \u200b =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] \u200b = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) \u200b =1-(0,242738+0,34483+0,2+0,2) \u200b =1-0,987567 \u200b =0,012433","title":"Decision Tree"},{"location":"DECISION TREE/#decision-tree","text":"Decision tree merupakan metode klarifikasi yang sering digunakan atau metode paling polpuler ,keunggulannya adalah mudah di interprestasi oleh manusia .dicision tree merupakan suatu prediksi yang berupa pohon atau bisa disebut stuktur beriharki,konsep decision tree adalah mengubah data yang ada menjadi pohon keputusan dan aturan aturan keputusan.","title":"DECISION TREE\u00b6"},{"location":"DECISION TREE/#cara-membuat-decision-tree","text":"Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain :","title":"CARA MEMBUAT DECISION TREE\u00b6"},{"location":"DECISION TREE/#entropy","text":"Entropy(S)=n\u2211i=1\u2212pilog2piEntropy(S)=\u2211i=1n\u2212pilog2pi keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S","title":"Entropy\u00b6"},{"location":"DECISION TREE/#gain","text":"kemudian hitung nilai gain menggunakan rumus :GAIN(S,A)=entropy(S)\u2212n\u2211i=1|Si||s|\u2217entropy(Si)GAIN(S,A)=entropy(S)\u2212\u2211i=1n|Si||s|\u2217entropy(Si)keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen (kolom, data): kelas=[] for i in range (len(data)): if data.values.tolist()[i][kolom] not in kelas: kelas.append(data.values.tolist()[i][kolom]) return kelas kelas=banyak_elemen(df.shape[1]-1, df) outlook=banyak_elemen(df.shape[1]-5,df) temp=banyak_elemen(df.shape[1]-4,df) humidity=banyak_elemen(df.shape[1]-3,df) windy=banyak_elemen(df.shape[1]-2,df) print(kelas,outlook,temp,humidity,windy)` ['no', 'yes'] ['sunny', 'overcast', 'rainy'] ['hot', 'mild', 'cool'] ['high', 'normal'] [False, True] Fungsi countvKelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas(kelas,kolomKelas,data): hasil=[] for x in range(len(kelas)): hasil.append(0) for i in range (len(data)): for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil pKelas=countvKelas(kelas,df.shape[1]-1,df) pKelas [5, 9] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy(T): hasil=0 jumlah=0 for y in T: jumlah+=y for z in range (len(T)): if jumlah!=0: T[z]=T[z]/jumlah for i in T: if i != 0: hasil-=i*math.log(i,2) return hasil def e_list(atribut,n): temp=[] tx=t_list(atribut,n) for i in range (len(atribut)): ent=entropy(tx[i]) temp.append(ent) return temp tOutlook=t_list(outlook,5) tTemp=t_list(temp,4) tHum=t_list(humidity,3) tWin=t_list(windy,2) print(\"Sunny, Overcast, Rainy\",eOutlook) print(\"Hot, Mild, Cold\", eTemp) print(\"High, Normal\", eHum) print(\"False, True\", eWin) Sunny, Overcast, Rainy [0.9709505944546686, 0.0, 0.9709505944546686] Hot, Mild, Cold [1.0, 0.9182958340544896, 0.8112781244591328] High, Normal [0.9852281360342516, 0.5916727785823275] False, True [0.8112781244591328, 1.0]","title":"Gain\u00b6"},{"location":"DECISION TREE/#contoh","text":"berikut contoh data yang akan di rubah menjadi decision tree <table border=\"1\" class=\"dataframe\"> <thead> 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari entropy(s) dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20Entropy(S)=n\u2211i=1\u2212pilog2piEntropy(S)=\u2211i=1n\u2212pilog2pi Entropy(S)=\u221210/20\u2217log210/20\u221210/20\u2217log210/20Entropy(S)=\u221210/20\u2217log210/20\u221210/20\u2217log210/20 Entropy(S)=1Entropy(S)=1 lalu kita menghitu gain setiap kolom di atas:GAIN(GENDER)=entropy(S)\u2212n\u2211i=1|Si||s|\u2217entropy(Si)GAIN(GENDER)=entropy(S)\u2212\u2211i=1n|Si||s|\u2217entropy(Si) GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] \u200b = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) \u200b =1-(10/20 x 0,970951)+(10/20 x 0,970951) \u200b =1-(0,4485475+0,4485475) \u200b =1-0,970951 \u200b =0.029049 GAIN(CARTIPE)=entropy(S)\u2212n\u2211i=1|Si||s|\u2217entropy(Si)GAIN(CARTIPE)=entropy(S)\u2212\u2211i=1n|Si||s|\u2217entropy(Si) GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] \u200b = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) \u200b =1-(0,162256+0+0,217426) \u200b =1-0,379681 \u200b =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] \u200b = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) \u200b =1-(0,242738+0,34483+0,2+0,2) \u200b =1-0,987567 \u200b =0,012433","title":"Contoh\u00b6"},{"location":"MengukurJarakData/","text":"Mengukur Jarak Data \u00b6 \u00b6 Mengukur Jarak Tipe Data \u00b6 \u00b6 Pengertian : \u00b6 \u00b6 Tantangan dalam era dan zaman ini salah satunya datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn,v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yixi,yi disebut atribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya Minkowski Distance \u00b6 \u00b6 Kelompok Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan dimana m adalah bilangan riel positif dan xi dan yi adalah dua vektor dalam ruang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar. Manhattan Distance \u00b6 \u00b6 Manhattan distance adalah kasus khusus dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. Bila ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan Euclidean Distance \u00b6 \u00b6 Euclidean Distance adalah jarak yang paling terkenal yang digunakan untuk data numerik. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini. Average Distance \u00b6 \u00b6 Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adalah versi modikfikasid dari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan Weighted Euclidean Distance \u00b6 \u00b6 Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan dimana wi adalah bobot yang diberikan pada atribut ke i. Chord Distance \u00b6 \u00b6 Chord Distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan dimana \u2016 x \u20162 adalah L 2-norm Mahalanobis Distance \u00b6 \u00b6 Mahalanobis Distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antara data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan dimana S adalah matrik covariance data. Consine measure \u00b6 \u00b6 Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan dimana \u2225y\u22252 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn)y=(y1,y2,\u2026,yn) didefinisikan dengan Pearson Correlation \u00b6 \u00b6 Pearson Correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan The Pearson correlation kelemahannya adalah sensitif terhadap outlier Mengukur Jarak Atribut Binary \u00b6 \u00b6 Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d72 di mana qq adalah jumlah atribut yang sama dengan 1 untuk kedua objek ii dan jj, rr adalah jumlah atribut yang sama dengan 1 untuk objek ii tetapi 0 untuk objek jj, ss adalah jumlah atribut yang sama dengan 0 untuk objek ii tetapi 1 untuk objek jj, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek ii dan jj. Jumlah total atribut adalah pp, di mana p=q+r+s+tp=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antarii dan j adalah $$ d ( i , j ) = \\frac { r + s } { q + r + s + t } $$ Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya $$ d ( i , j ) = \\frac { r + s } { q + r + s } $$ Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek ii dan jj dapat dihitung dengan $$ \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) $$ Persamaan similarity ini disebut dengan Jaccard coefficient Mengukur Jarak Tipe categorical \u00b6 \u00b6 Overlay Metric \u00b6 \u00b6 Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) $$ dimana nn adalah banyaknya atribut, ai(x)ai(x) dan ai(y)ai(y) adalah nilai atribut ke ii yaitu AiAi dari masing masing objek xx dan yy, \u03b4 (ai(x),ai(y))\u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi. Value Difference Metric (VDM) \u00b6 \u00b6 VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | $$ dimana CCadalah banyaknya kelas, P(c|ai(x))P(c|ai(x)) adalah probabilitas bersyarat dimana kelas xx adalah cc dari atribut AiAi, yang memiliki nilai ai(x)ai(x), P(c|ai(y))P(c|ai(y)) adalah probabilitas bersyarat dimana kelas yy adalah cc dengan atribut AiAi memiliki nilai ai(y)ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| $$ diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) dan P(c|y) didekati dengan Naive Bayes, Minimum Risk Metric (MRM) \u00b6 \u00b6 Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$ Mengukur Jarak Tipe Ordinal \u00b6 \u00b6 Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal ff yang memiliki MfMf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. MM adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan ff adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: \u00b7 Nilai ff untuk objek ke-ii adalah xifxif, dan ff memiliki MfMf status urutan , mewakili peringkat 1,..,Mf1,..,Mf Ganti setiap xifxif dengan peringkatnya, rif\u2208{1...Mf}rif\u2208{1...Mf} \u00b7 Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rifrif dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ \u00b7 Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$ Menghitung Jarak Tipe Campuran \u00b6 \u00b6 Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar 0,0,1.0. Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan $$ d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } $$ dimana \u03b4fij=0\u03b4ijf=0 - jika xifxif atau xjfxjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek ii atau objek jj) \u00b7 jika xif=xjf=0xif=xjf=0 dan \u00b7 atribut ff adalah binary asymmetric, selain itu \u03b4fij=1\u03b4ijf=1 Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu.dfijdijf) dihitung bergantung pada tipenya, \u00b7 Jika ff adalah numerik, $$ d_{ij}^{f}=\\frac{ |x {if}-x {jf}|}{max_hx_{hf}-min_hx{hf}} $$ , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f \u00b7 Jika ff adalah nominal atau binary,$d_{ij}^{f}=0 $jika xif=xjfxif=xjf, sebaliknya dfij=1dijf=1 \u00b7 Jika ff adalah ordinal maka hitung rangking rifrif dan $$ \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} $$ , dan perlakukan zifzif sebagai numerik. Langkah - langkah Mengukur Jarak \u00b6 \u00b6 Alat dan Bahan \u00b6 \u00b6 library python yang digunakan adalah sebagai berikut : pandas, digunakan untuk data manajemen dan data analysis. scipy, merupakan library berisi kumpulan algoritma dan fungsi matematika. Langkah - langkah \u00b6 \u00b6 PERTAMA : \u00b6 \u00b6 pertama - tama download datanya https://archive.ics.uci.edu/ml/machine-learning-n ndatabases/haberman/ KEDUA : \u00b6 \u00b6 selanjutnya masukkan library yang telah disiapkan from scipy import stats import pandas as pd KETIGA : \u00b6 \u00b6 Selanjutnya memuat data csvnya df = pd.read_csv('data jarak.csv') df setelah di run : 30 64 1 1.1 0 30 62 3 1 1 30 65 0 1 2 31 59 2 1 3 31 65 4 1 4 33 58 10 1 5 33 60 0 1 6 34 59 0 2 7 34 66 9 2 8 34 58 30 1 9 34 60 1 1 10 34 61 10 1 11 34 67 7 1 12 34 60 0 1 13 35 64 13 1 14 35 63 0 1 15 36 60 1 1 16 36 69 0 1 17 37 60 0 1 18 37 63 0 1 19 37 58 0 1 20 37 59 6 1 21 37 60 15 1 22 37 63 0 1 23 38 69 21 2 24 38 59 2 1 25 38 60 0 1 26 38 60 0 1 27 38 62 3 1 28 38 64 1 1 29 38 66 0 1 ... ... ... ... ... 275 67 66 0 1 276 67 61 0 1 277 67 65 0 1 278 68 67 0 1 279 68 68 0 1 280 69 67 8 2 281 69 60 0 1 282 69 65 0 1 283 69 66 0 1 284 70 58 0 2 285 70 58 4 2 286 70 66 14 1 287 70 67 0 1 288 70 68 0 1 289 70 59 8 1 290 70 63 0 1 291 71 68 2 1 292 72 63 0 2 293 72 58 0 1 294 72 64 0 1 295 72 67 3 1 296 73 62 0 1 297 73 68 0 1 298 74 65 3 2 299 74 63 0 1 300 75 62 1 1 301 76 67 0 1 302 77 65 3 1 303 78 65 1 2 304 83 58 2 2 KEEMPAT : \u00b6 \u00b6 Kemudian membuat data penyimpanan / dictionary yang menampung nilai yang ditampilkan. selanjutnya mengambil data dari beberapa kolom pada csv dengan cara diiterasi serta, menghitungnya dengan berbagai metode yang telah disiapkan oleh pandas itu sendiri. kemudian hasil tersebut di disimpan pada penyimpanan yang tadi, dan memvisualisasikan hasil tersebut dalam bentuk data frame df = pd.read_csv('data jarak.csv') k=df.iloc[10:16] k setelah di run: 30 64 1 1.1 10 34 61 10 1 11 34 67 7 1 12 34 60 0 1 13 35 64 13 1 14 35 63 0 1 15 36 60 1 1 KELIMA : \u00b6 \u00b6 numerical=[0,3] categorical=[1,2,6,7] binary=[4,5,8] ordinal=[1,2] from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Ordinal\"]+[\"Categorical\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[0]+[0]+[0]+[0], [\"v1-v3\"]+[0]+[0]+[0]+[0]+[0], [\"v2-v3\"]+[0]+[0]+[0]+[0]+[0], [\"v3-v4\"]+[0]+[0]+[0]+[0]+[0], [\"v4-v5\"]+[0]+[0]+[0]+[0]+[0], [\"v5-v6\"]+[0]+[0]+[0]+[0]+[0] ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) setelah di run: Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 0 0 0 0 v1-v3 0 0 0 0 0 v2-v3 0 0 0 0 0 v3-v4 0 0 0 0 0 v4-v5 0 0 0 0 0 v5-v6 0 0 0 0 0 Mengukur Jarak Numerik \u00b6 \u00b6 def chordDist(v1,v2,jnis): jmlh=0 normv1=0 normv2=0 for x in range (len(jnis)): normv1=normv1+(int(k.values.tolist()[v1][jnis[x]])**2) normv2=normv2+(int(k.values.tolist()[v1][jnis[x]])**2) jmlh=jmlh+(int(k.values.tolist()[v1][jnis[x]])*int(k.values.tolist()[v2][jnis[x]])) return ((2-(2*jmlh/(normv1*normv2)))**0.5) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Ordinal\"]+[\"Categorical\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[\"{:.2f}\".format(chordDist(0,1,numerical))]+[0]+[0]+[0], [\"v1-v3\"]+[0]+[\"{:.2f}\".format(chordDist(0,2,numerical))]+[0]+[0]+[0], [\"v2-v3\"]+[0]+[\"{:.2f}\".format(chordDist(1,2,numerical))]+[0]+[0]+[0], [\"v3-v4\"]+[0]+[\"{:.2f}\".format(chordDist(2,3,numerical))]+[0]+[0]+[0], [\"v4-v5\"]+[0]+[\"{:.2f}\".format(chordDist(3,4,numerical))]+[0]+[0]+[0], [\"v5-v6\"]+[0]+[\"{:.2f}\".format(chordDist(2,3,numerical))]+[0]+[0]+[0], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 1.41 0 0 0 v1-v3 0 1.41 0 0 0 v2-v3 0 1.41 0 0 0 v3-v4 0 1.41 0 0 0 v4-v5 0 1.41 0 0 0 v5-v6 0 1.41 0 0 0","title":"Mengukur Jarak Data"},{"location":"MengukurJarakData/#mengukur-jarak-data","text":"","title":"Mengukur Jarak Data\u00b6"},{"location":"MengukurJarakData/#mengukur-jarak-tipe-data","text":"","title":"Mengukur Jarak Tipe Data\u00b6"},{"location":"MengukurJarakData/#pengertian","text":"Tantangan dalam era dan zaman ini salah satunya datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn,v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yixi,yi disebut atribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya","title":"Pengertian :\u00b6"},{"location":"MengukurJarakData/#minkowski-distance","text":"Kelompok Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan dimana m adalah bilangan riel positif dan xi dan yi adalah dua vektor dalam ruang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar.","title":"Minkowski Distance\u00b6"},{"location":"MengukurJarakData/#manhattan-distance","text":"Manhattan distance adalah kasus khusus dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. Bila ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan","title":"Manhattan Distance\u00b6"},{"location":"MengukurJarakData/#euclidean-distance","text":"Euclidean Distance adalah jarak yang paling terkenal yang digunakan untuk data numerik. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini.","title":"Euclidean Distance\u00b6"},{"location":"MengukurJarakData/#average-distance","text":"Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adalah versi modikfikasid dari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan","title":"Average Distance\u00b6"},{"location":"MengukurJarakData/#weighted-euclidean-distance","text":"Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan dimana wi adalah bobot yang diberikan pada atribut ke i.","title":"Weighted Euclidean Distance\u00b6"},{"location":"MengukurJarakData/#chord-distance","text":"Chord Distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan dimana \u2016 x \u20162 adalah L 2-norm","title":"Chord Distance\u00b6"},{"location":"MengukurJarakData/#mahalanobis-distance","text":"Mahalanobis Distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antara data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan dimana S adalah matrik covariance data.","title":"Mahalanobis Distance\u00b6"},{"location":"MengukurJarakData/#consine-measure","text":"Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan dimana \u2225y\u22252 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn)y=(y1,y2,\u2026,yn) didefinisikan dengan","title":"Consine measure\u00b6"},{"location":"MengukurJarakData/#pearson-correlation","text":"Pearson Correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan The Pearson correlation kelemahannya adalah sensitif terhadap outlier","title":"Pearson Correlation\u00b6"},{"location":"MengukurJarakData/#mengukur-jarak-atribut-binary","text":"Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d72 di mana qq adalah jumlah atribut yang sama dengan 1 untuk kedua objek ii dan jj, rr adalah jumlah atribut yang sama dengan 1 untuk objek ii tetapi 0 untuk objek jj, ss adalah jumlah atribut yang sama dengan 0 untuk objek ii tetapi 1 untuk objek jj, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek ii dan jj. Jumlah total atribut adalah pp, di mana p=q+r+s+tp=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antarii dan j adalah $$ d ( i , j ) = \\frac { r + s } { q + r + s + t } $$ Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya $$ d ( i , j ) = \\frac { r + s } { q + r + s } $$ Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek ii dan jj dapat dihitung dengan $$ \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) $$ Persamaan similarity ini disebut dengan Jaccard coefficient","title":"Mengukur Jarak Atribut Binary\u00b6"},{"location":"MengukurJarakData/#mengukur-jarak-tipe-categorical","text":"","title":"Mengukur Jarak Tipe categorical\u00b6"},{"location":"MengukurJarakData/#overlay-metric","text":"Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) $$ dimana nn adalah banyaknya atribut, ai(x)ai(x) dan ai(y)ai(y) adalah nilai atribut ke ii yaitu AiAi dari masing masing objek xx dan yy, \u03b4 (ai(x),ai(y))\u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi.","title":"Overlay Metric\u00b6"},{"location":"MengukurJarakData/#value-difference-metric-vdm","text":"VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | $$ dimana CCadalah banyaknya kelas, P(c|ai(x))P(c|ai(x)) adalah probabilitas bersyarat dimana kelas xx adalah cc dari atribut AiAi, yang memiliki nilai ai(x)ai(x), P(c|ai(y))P(c|ai(y)) adalah probabilitas bersyarat dimana kelas yy adalah cc dengan atribut AiAi memiliki nilai ai(y)ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| $$ diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) dan P(c|y) didekati dengan Naive Bayes,","title":"Value Difference Metric (VDM)\u00b6"},{"location":"MengukurJarakData/#minimum-risk-metric-mrm","text":"Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$","title":"Minimum Risk Metric (MRM)\u00b6"},{"location":"MengukurJarakData/#mengukur-jarak-tipe-ordinal","text":"Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal ff yang memiliki MfMf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. MM adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan ff adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: \u00b7 Nilai ff untuk objek ke-ii adalah xifxif, dan ff memiliki MfMf status urutan , mewakili peringkat 1,..,Mf1,..,Mf Ganti setiap xifxif dengan peringkatnya, rif\u2208{1...Mf}rif\u2208{1...Mf} \u00b7 Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rifrif dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ \u00b7 Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$","title":"Mengukur Jarak Tipe Ordinal\u00b6"},{"location":"MengukurJarakData/#menghitung-jarak-tipe-campuran","text":"Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar 0,0,1.0. Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan $$ d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } $$ dimana \u03b4fij=0\u03b4ijf=0 - jika xifxif atau xjfxjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek ii atau objek jj) \u00b7 jika xif=xjf=0xif=xjf=0 dan \u00b7 atribut ff adalah binary asymmetric, selain itu \u03b4fij=1\u03b4ijf=1 Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu.dfijdijf) dihitung bergantung pada tipenya, \u00b7 Jika ff adalah numerik, $$ d_{ij}^{f}=\\frac{ |x {if}-x {jf}|}{max_hx_{hf}-min_hx{hf}} $$ , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f \u00b7 Jika ff adalah nominal atau binary,$d_{ij}^{f}=0 $jika xif=xjfxif=xjf, sebaliknya dfij=1dijf=1 \u00b7 Jika ff adalah ordinal maka hitung rangking rifrif dan $$ \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} $$ , dan perlakukan zifzif sebagai numerik.","title":"Menghitung Jarak Tipe Campuran\u00b6"},{"location":"MengukurJarakData/#langkah-langkah-mengukur-jarak","text":"","title":"Langkah - langkah Mengukur Jarak\u00b6"},{"location":"MengukurJarakData/#alat-dan-bahan","text":"library python yang digunakan adalah sebagai berikut : pandas, digunakan untuk data manajemen dan data analysis. scipy, merupakan library berisi kumpulan algoritma dan fungsi matematika.","title":"Alat dan Bahan\u00b6"},{"location":"MengukurJarakData/#langkah-langkah","text":"","title":"Langkah - langkah\u00b6"},{"location":"MengukurJarakData/#pertama","text":"pertama - tama download datanya https://archive.ics.uci.edu/ml/machine-learning-n ndatabases/haberman/","title":"PERTAMA :\u00b6"},{"location":"MengukurJarakData/#kedua","text":"selanjutnya masukkan library yang telah disiapkan from scipy import stats import pandas as pd","title":"KEDUA :\u00b6"},{"location":"MengukurJarakData/#ketiga","text":"Selanjutnya memuat data csvnya df = pd.read_csv('data jarak.csv') df setelah di run : 30 64 1 1.1 0 30 62 3 1 1 30 65 0 1 2 31 59 2 1 3 31 65 4 1 4 33 58 10 1 5 33 60 0 1 6 34 59 0 2 7 34 66 9 2 8 34 58 30 1 9 34 60 1 1 10 34 61 10 1 11 34 67 7 1 12 34 60 0 1 13 35 64 13 1 14 35 63 0 1 15 36 60 1 1 16 36 69 0 1 17 37 60 0 1 18 37 63 0 1 19 37 58 0 1 20 37 59 6 1 21 37 60 15 1 22 37 63 0 1 23 38 69 21 2 24 38 59 2 1 25 38 60 0 1 26 38 60 0 1 27 38 62 3 1 28 38 64 1 1 29 38 66 0 1 ... ... ... ... ... 275 67 66 0 1 276 67 61 0 1 277 67 65 0 1 278 68 67 0 1 279 68 68 0 1 280 69 67 8 2 281 69 60 0 1 282 69 65 0 1 283 69 66 0 1 284 70 58 0 2 285 70 58 4 2 286 70 66 14 1 287 70 67 0 1 288 70 68 0 1 289 70 59 8 1 290 70 63 0 1 291 71 68 2 1 292 72 63 0 2 293 72 58 0 1 294 72 64 0 1 295 72 67 3 1 296 73 62 0 1 297 73 68 0 1 298 74 65 3 2 299 74 63 0 1 300 75 62 1 1 301 76 67 0 1 302 77 65 3 1 303 78 65 1 2 304 83 58 2 2","title":"KETIGA :\u00b6"},{"location":"MengukurJarakData/#keempat","text":"Kemudian membuat data penyimpanan / dictionary yang menampung nilai yang ditampilkan. selanjutnya mengambil data dari beberapa kolom pada csv dengan cara diiterasi serta, menghitungnya dengan berbagai metode yang telah disiapkan oleh pandas itu sendiri. kemudian hasil tersebut di disimpan pada penyimpanan yang tadi, dan memvisualisasikan hasil tersebut dalam bentuk data frame df = pd.read_csv('data jarak.csv') k=df.iloc[10:16] k setelah di run: 30 64 1 1.1 10 34 61 10 1 11 34 67 7 1 12 34 60 0 1 13 35 64 13 1 14 35 63 0 1 15 36 60 1 1","title":"KEEMPAT :\u00b6"},{"location":"MengukurJarakData/#kelima","text":"numerical=[0,3] categorical=[1,2,6,7] binary=[4,5,8] ordinal=[1,2] from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Ordinal\"]+[\"Categorical\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[0]+[0]+[0]+[0], [\"v1-v3\"]+[0]+[0]+[0]+[0]+[0], [\"v2-v3\"]+[0]+[0]+[0]+[0]+[0], [\"v3-v4\"]+[0]+[0]+[0]+[0]+[0], [\"v4-v5\"]+[0]+[0]+[0]+[0]+[0], [\"v5-v6\"]+[0]+[0]+[0]+[0]+[0] ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) setelah di run: Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 0 0 0 0 v1-v3 0 0 0 0 0 v2-v3 0 0 0 0 0 v3-v4 0 0 0 0 0 v4-v5 0 0 0 0 0 v5-v6 0 0 0 0 0","title":"KELIMA :\u00b6"},{"location":"MengukurJarakData/#mengukur-jarak-numerik","text":"def chordDist(v1,v2,jnis): jmlh=0 normv1=0 normv2=0 for x in range (len(jnis)): normv1=normv1+(int(k.values.tolist()[v1][jnis[x]])**2) normv2=normv2+(int(k.values.tolist()[v1][jnis[x]])**2) jmlh=jmlh+(int(k.values.tolist()[v1][jnis[x]])*int(k.values.tolist()[v2][jnis[x]])) return ((2-(2*jmlh/(normv1*normv2)))**0.5) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Ordinal\"]+[\"Categorical\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[\"{:.2f}\".format(chordDist(0,1,numerical))]+[0]+[0]+[0], [\"v1-v3\"]+[0]+[\"{:.2f}\".format(chordDist(0,2,numerical))]+[0]+[0]+[0], [\"v2-v3\"]+[0]+[\"{:.2f}\".format(chordDist(1,2,numerical))]+[0]+[0]+[0], [\"v3-v4\"]+[0]+[\"{:.2f}\".format(chordDist(2,3,numerical))]+[0]+[0]+[0], [\"v4-v5\"]+[0]+[\"{:.2f}\".format(chordDist(3,4,numerical))]+[0]+[0]+[0], [\"v5-v6\"]+[0]+[\"{:.2f}\".format(chordDist(2,3,numerical))]+[0]+[0]+[0], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 1.41 0 0 0 v1-v3 0 1.41 0 0 0 v2-v3 0 1.41 0 0 0 v3-v4 0 1.41 0 0 0 v4-v5 0 1.41 0 0 0 v5-v6 0 1.41 0 0 0","title":"Mengukur Jarak Numerik\u00b6"},{"location":"MissingValues/","text":"Missing Values using KNN \u00b6 \u00b6 KNN adalah algoritma yang berguna untuk mencocokkan suatu titik dengan tetangga terdekatnya dalam ruang multi-dimensi. Ini dapat digunakan untuk data yang kontinu, diskrit, ordinal, dan kategoris yang membuatnya sangat berguna untuk menangani semua jenis data yang hilang. Asumsi di balik menggunakan KNN untuk nilai yang hilang adalah bahwa nilai poin dapat didekati dengan nilai dari poin yang paling dekat dengannya, berdasarkan pada variabel lain. Mari kita simpan contoh sebelumnya dan tambahkan variabel lain, penghasilan orang tersebut. Sekarang kami memiliki tiga variabel, jenis kelamin, pendapatan dan tingkat depresi yang memiliki nilai yang hilang. Kami kemudian berasumsi bahwa orang-orang dengan pendapatan yang sama dan jenis kelamin yang sama cenderung memiliki tingkat depresi yang sama. Untuk nilai yang hilang, kita akan melihat jenis kelamin orang tersebut, pendapatannya, mencari k tetangga terdekatnya dan mendapatkan tingkat depresi mereka. Kita kemudian dapat memperkirakan tingkat depresi orang yang kita inginkan. Kalibrasi Parameter KNN \u00b6 \u00b6 Jumlah tetangga yang harus dicari \u00b6 \u00b6 Mengambil k rendah akan meningkatkan pengaruh kebisingan dan hasilnya akan kurang digeneralisasikan. Di sisi lain, mengambil k tinggi akan cenderung mengaburkan efek lokal yang persis apa yang kita cari. Juga disarankan untuk mengambil k yang aneh untuk kelas biner untuk menghindari ikatan. Metode agregasi untuk digunakan \u00b6 \u00b6 Di sini kita memungkinkan untuk mean aritmatika, median dan mode untuk variabel numerik dan mode untuk yang kategorikal Normalisasi data \u00b6 \u00b6 Ini adalah metode yang memungkinkan setiap atribut memberikan pengaruh yang sama dalam mengidentifikasi tetangga saat menghitung jenis jarak tertentu seperti yang Euclidean. Anda harus menormalkan data Anda ketika skala tidak memiliki arti dan / atau Anda memiliki skala tidak konsisten seperti sentimeter dan meter. Ini menyiratkan pengetahuan sebelumnya tentang data untuk mengetahui mana yang lebih penting. Algoritma secara otomatis menormalkan data ketika variabel numerik dan kategorikal disediakan. Atribut numerik jarak \u00b6 \u00b6 Di antara berbagai metrik jarak yang tersedia, kami akan fokus pada yang utama, Euclidean dan Manhattan. Euclidean adalah ukuran jarak yang baik untuk digunakan jika variabel input bertipe sama (mis. Semua lebar dan tinggi yang diukur). Jarak Manhattan adalah ukuran yang baik untuk digunakan jika variabel input tidak dalam jenis yang sama (seperti usia, tinggi, dll ...). Atribut kategorikal jarak \u00b6 \u00b6 tanpa transformasi sebelumnya, jarak yang berlaku terkait dengan frekuensi dan kesamaan. Atribut kategorikal hampir sama dengan nominal karena dengan tipe ini akan dinormalisasikan menjadi numerik atau angka untuk bisa dirukur jaraknya # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = {'First Score':[100, 90, np.nan, 95], 'Second Score': [30, 45, 56, np.nan], 'Third Score':[np.nan, 40, 80, 98]} # creating a dataframe from dictionary df = pd.DataFrame(dict) # filling missing value using fillna() df.fillna(0) setelah dirun: First Score Second Score Third Score 0 100.0 30.0 0.0 1 90.0 45.0 40.0 2 0.0 56.0 80.0 3 95.0 0.0 98.0","title":"Missing Values KNN"},{"location":"MissingValues/#missing-values-using-knn","text":"KNN adalah algoritma yang berguna untuk mencocokkan suatu titik dengan tetangga terdekatnya dalam ruang multi-dimensi. Ini dapat digunakan untuk data yang kontinu, diskrit, ordinal, dan kategoris yang membuatnya sangat berguna untuk menangani semua jenis data yang hilang. Asumsi di balik menggunakan KNN untuk nilai yang hilang adalah bahwa nilai poin dapat didekati dengan nilai dari poin yang paling dekat dengannya, berdasarkan pada variabel lain. Mari kita simpan contoh sebelumnya dan tambahkan variabel lain, penghasilan orang tersebut. Sekarang kami memiliki tiga variabel, jenis kelamin, pendapatan dan tingkat depresi yang memiliki nilai yang hilang. Kami kemudian berasumsi bahwa orang-orang dengan pendapatan yang sama dan jenis kelamin yang sama cenderung memiliki tingkat depresi yang sama. Untuk nilai yang hilang, kita akan melihat jenis kelamin orang tersebut, pendapatannya, mencari k tetangga terdekatnya dan mendapatkan tingkat depresi mereka. Kita kemudian dapat memperkirakan tingkat depresi orang yang kita inginkan.","title":"Missing Values using KNN\u00b6"},{"location":"MissingValues/#kalibrasi-parameter-knn","text":"","title":"Kalibrasi Parameter KNN\u00b6"},{"location":"MissingValues/#jumlah-tetangga-yang-harus-dicari","text":"Mengambil k rendah akan meningkatkan pengaruh kebisingan dan hasilnya akan kurang digeneralisasikan. Di sisi lain, mengambil k tinggi akan cenderung mengaburkan efek lokal yang persis apa yang kita cari. Juga disarankan untuk mengambil k yang aneh untuk kelas biner untuk menghindari ikatan.","title":"Jumlah tetangga yang harus dicari\u00b6"},{"location":"MissingValues/#metode-agregasi-untuk-digunakan","text":"Di sini kita memungkinkan untuk mean aritmatika, median dan mode untuk variabel numerik dan mode untuk yang kategorikal","title":"Metode agregasi untuk digunakan\u00b6"},{"location":"MissingValues/#normalisasi-data","text":"Ini adalah metode yang memungkinkan setiap atribut memberikan pengaruh yang sama dalam mengidentifikasi tetangga saat menghitung jenis jarak tertentu seperti yang Euclidean. Anda harus menormalkan data Anda ketika skala tidak memiliki arti dan / atau Anda memiliki skala tidak konsisten seperti sentimeter dan meter. Ini menyiratkan pengetahuan sebelumnya tentang data untuk mengetahui mana yang lebih penting. Algoritma secara otomatis menormalkan data ketika variabel numerik dan kategorikal disediakan.","title":"Normalisasi data\u00b6"},{"location":"MissingValues/#atribut-numerik-jarak","text":"Di antara berbagai metrik jarak yang tersedia, kami akan fokus pada yang utama, Euclidean dan Manhattan. Euclidean adalah ukuran jarak yang baik untuk digunakan jika variabel input bertipe sama (mis. Semua lebar dan tinggi yang diukur). Jarak Manhattan adalah ukuran yang baik untuk digunakan jika variabel input tidak dalam jenis yang sama (seperti usia, tinggi, dll ...).","title":"Atribut numerik jarak\u00b6"},{"location":"MissingValues/#atribut-kategorikal-jarak","text":"tanpa transformasi sebelumnya, jarak yang berlaku terkait dengan frekuensi dan kesamaan. Atribut kategorikal hampir sama dengan nominal karena dengan tipe ini akan dinormalisasikan menjadi numerik atau angka untuk bisa dirukur jaraknya # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = {'First Score':[100, 90, np.nan, 95], 'Second Score': [30, 45, 56, np.nan], 'Third Score':[np.nan, 40, 80, 98]} # creating a dataframe from dictionary df = pd.DataFrame(dict) # filling missing value using fillna() df.fillna(0) setelah dirun: First Score Second Score Third Score 0 100.0 30.0 0.0 1 90.0 45.0 40.0 2 0.0 56.0 80.0 3 95.0 0.0 98.0","title":"Atribut kategorikal jarak\u00b6"},{"location":"StatistikaDeskriptif/","text":"Statistika Deskriptif \u00b6 \u00b6 Pengertian : \u00b6 \u00b6 Statistika Deskriptif adalah metode representasi keseluruhan himpunan data spesifik dengan memberikan ringkasan pendek tentang sampel dan ukuran data Statistika Deskriptif juga merupakan metode yang sangat sederhana karena hanya mendeskripsikan kondisi dari data yang dimiliki dan menyajikannya dalam bentuk tabel diagram grafik dan bentuk lainnya yang di tampilkan dalam uraian singkat dan terbatas, sehingga dapat memberikan informasi yang berguna. Tipe Statistik Deskriptif : \u00b6 \u00b6 Mean \u00b6 \u00b6 Mean merupakan rata-rata dari keseluruhan angka. Mean didapatkan dari hasil penjumlahan dari keseluruhan angka yang dibagi dengan banyaknya angka itu sendiri. Untuk menghitung data, kita misalkan N data dengan rumus berikut :\u00afx=n\u2211i=1xiN=x1+x2+x3+...+xnNx\u00af=\u2211i=1nxiN=x1+x2+x3+...+xnNKeterangan : x bar = x rata-rata = nilai rata-rata sampel x = data ke n n = banyak data Median \u00b6 \u00b6 Median merupakan nilai tengah dari sebuah urutan data, median disimbolkan dengan Me. Nilai dari median akan sama dengan nilai Quartil 2 / Q2. dalam mencari median yang banyak n dari data ganjil dan genap memiliki cara perhitungan data yang berbeda, dengan rumus sebagai berikut :Me=Q2=(n+12),jikanganjilMe=Q2=(n+12),jikanganjil Me=Q2=(xn2xn+122),jikangenapMe=Q2=(xn2xn+122),jikangenap Keterangan : Me = Nilai tengah dari kelompok data n = banyak data Modus \u00b6 \u00b6 Modus merupakan nilai / angka yang paling sering ditemukan dalam sebuah kelompok angka, atau data yang paling sering muncul, atau memiliki frekuensi tertinggi. Modus dilambangkan dengan Mo. dapat dihitung denganrumus berikut :Mo=Tb+pb1b1+b2Mo=Tb+pb1b1+b2keterangan : Mo = modus dari kelompok data Tb = tepi bawah dari elemen modus b1 = selisih frekuensi antara elemen modus dengan elemet sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen sesudahnya p = panjang interval nilai b1 dan b2 \u2013> adalah mutlak / selalu positif Varians \u00b6 \u00b6 Varians adalah ukuran penyebaran setiap nilai dalam suatu himpunan data dari rata-rata. Dalam proses mencari varian terdapat langkah yang harus dilakukan, dengan mengambil ukuran jarak dari setiap nilai dan mengurangi rata-rata dari setiap nilai dalam data, kemudian hasil dari ukuran jarak tersebut dikuadratkan dan membagi jumlah kuadrat dengan jumlah nilai dalam himpunan data. Dapat dihitung dengan rumus berikut :\u03c32=n\u2211i=1(xi\u2212\u00afx)2n\u03c32=\u2211i=1n(xi\u2212x\u00af)2nKeterangan : Xi = titik data x bar = rata-rata dari semua titik data n = banyak dari dari anggota data Standar Deviasi \u00b6 \u00b6 Standar Deviasi merupakan simpanan baku atau ukuran dispersi kumpulan data relatif terhadap rata-rata atau lebih simpelnya adalah akar kuadrat positif dari varian. Standar deviasi dihitung dengan mengakar kuadratkan nilai dari varians. jika titik data lebih dari rata-rata dalam kumpulan data maka semakin tinggi standar deviasi. dapat dihitung dengan rumus berikut :\u03c3=\ue001\ue000 \ue000 \ue000\u23b7n\u2211i=1(xi\u2212\u00afx)2n\u03c3=\u2211i=1n(xi\u2212x\u00af)2nKeterangan : xi = nilai x ke i x = rata rata n = ukuran sampel Skewness \u00b6 \u00b6 Skewness merupakan kemiringan atau ketidak simetrisan pada suatu distribusi statistik dimana kurva tampak condong ke kiri / ke kanan. Skewness digunakan untuk menentukan sejauh mana perbedaan suatu distribusi dengan distribusi normal. Dalam distribusi normal grafik muncul seperti kurva berbentuk lonceng. ketika suatu distribusi mengalami kemiringan ke sebelah kanan dan ekor di sisi kanan kurva lebih panjang dari ekor sisi kiri kurva maka situasi ini dikatakan kemiringan positif dan sebaliknya dikatakan kemiringan negative. Skewness dapat dihitung menggunakan rumus sebagai berikut:Skewness=\u2211i=1n(xi\u2212\u00afx)i(n\u22121)\u03c33Skewness=\u2211i=1n(xi\u2212x\u00af)i(n\u22121)\u03c33Keterangan : xi= titik data x bar = rata-rata dari distribusi n = jumlah titik dalam distribusi o = standar deviasi Quartile Quartile merupakan nilai-nilai yang membagi data yang telah diurutkan kedalam 4 bagian yang sama besar. Kuartil dinotasikan dengan notasi Q. Kuartil terdiri dari 3, yaitu kuartil pertama Q1, kuartil kedua Q2, dan kuartil ketiga Q3. Untuk menentukan kuartil pada data tunggal, kita harus mempertimbangkan banyaknya data n terlebih dahulu. Penghitungan quartil tergantung dari kondisi banyaknya data tersebut, Dalam mencari quatile kita dapat menggunakan rumus berikut ini:Q1=(n+1)14Q1=(n+1)14 Q2=(n+1)12Q2=(n+1)12 Q3=(n+1)34Q3=(n+1)34 Keterangan : Q = nilai quartil n = banyak data Penerapan Statistik Deskriptif Menggunakan Python \u00b6 \u00b6 Alat dan Bahan : \u00b6 \u00b6 Pada penerapan ini saya menggunakan 100 data random yang disimpan dalam bentuk .csv dan untuk mempermudah dalam penerapan tersebut, perlu disiapkan library python yang dapat didownload secara gratis. library python yang digunakan adalah sebagai berikut : pandas, digunakan untuk data manajemen dan data analysis. scipy, merupakan library berisi kumpulan algoritma dan fungsi matematika. Langkah - Langkah \u00b6 \u00b6 PERTAMA \u00b6 \u00b6 Pertama-tama masukkan library yang sudah disiapkan from scipy import stats import pandas as pd KEDUA \u00b6 \u00b6 Selanjutnya memuat data csv yang telah disiapkan df = pd.read_csv('data.csv') df setelah di run maka program akan menampilkan seperti berikut : tinggi badan berat badan tekanan darah lingkar perut 0 165 53 112 50 1 152 66 104 69 2 171 55 120 42 3 155 59 127 63 4 158 49 111 47 5 173 51 103 51 6 165 71 119 47 7 157 46 123 43 8 169 80 101 62 9 160 48 130 41 10 157 62 127 60 11 157 60 106 47 12 151 67 129 70 13 177 43 111 48 14 159 52 119 43 15 179 64 117 47 16 163 76 106 48 17 164 57 113 48 18 155 61 121 41 19 174 67 124 66 20 154 52 122 61 21 155 45 117 58 22 165 49 108 51 23 168 72 127 57 24 165 77 116 56 25 161 49 113 43 26 156 64 127 57 27 166 49 125 68 28 164 78 129 63 29 151 46 100 57 ... ... ... ... ... 70 170 73 107 42 71 157 70 105 63 72 180 44 100 54 73 162 49 116 62 74 171 62 103 41 75 169 42 126 59 76 165 51 111 66 77 157 61 109 56 78 163 79 101 65 79 151 58 119 60 80 161 54 129 40 81 177 72 106 63 82 179 46 117 40 83 162 78 106 41 84 172 69 116 60 85 167 67 115 44 86 162 54 117 55 87 170 72 128 59 88 159 78 124 44 89 178 43 105 48 90 177 79 111 67 91 165 62 106 70 92 170 56 119 66 93 150 59 117 63 94 163 65 115 58 95 164 76 129 45 96 160 55 129 49 97 157 40 124 49 98 156 47 123 61 99 162 65 126 67 100 rows \u00d7 4 columns KETIGA \u00b6 \u00b6 Kemudian membuat data penyimpanan / dictionary yang menampung nilai yang ditampilkan. selanjutnya mengambil data dari beberapa kolom pada csv dengan cara diiterasi serta, menghitungnya dengan berbagai metode yang telah disiapkan oleh pandas itu sendiri. kemudian hasil tersebut di disimpan pada penyimpanan yang tadi. from IPython.display import HTML, display import tabulate table=[ [\"method\"]+[x for x in df.columns], [\"count()\"]+[df[col].count() for col in df.columns], [\"mean()\"]+[df[col].mean() for col in df.columns], [\"std()\"]+[\"{:.2f}\".format(df[col].std()) for col in df.columns], [\"min()\"]+[df[col].min() for col in df.columns], [\"max()\"]+[df[col].max() for col in df.columns], [\"q1()\"]+[df[col].quantile(0.25) for col in df.columns], [\"q2()\"]+[df[col].quantile(0.50) for col in df.columns], [\"q3\"]+[df[col].quantile(0.75) for col in df.columns], [\"skew\"]+[\"{:.2f}\".format(df[col].skew()) for col in df.columns], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) KEEMPAT \u00b6 \u00b6 Yang terakhir yaitu memvisualisasikan hasil tersebut dalam bentuk data frame display(HTML(tabulate.tabulate(table, tablefmt='html'))) setelah di run maka program akan menampilkan seperti berikut : method tinggi badan berat badan tekanan darah lingkar perut count() 100 100 100 100 mean() 164.8 60.43 114.88 55.51 std() 7.86 11.81 8.81 9.59 min() 150 40 100 40 max() 180 80 130 70 q1() 158.0 49.75 106.75 47.0 q2() 165.0 60.5 115.5 57.0 q3 171.0 71.0 122.25 63.25 skew 0.07 0.06 -0.04 -0.15","title":"statistika deskriptif"},{"location":"StatistikaDeskriptif/#statistika-deskriptif","text":"","title":"Statistika Deskriptif\u00b6"},{"location":"StatistikaDeskriptif/#pengertian","text":"Statistika Deskriptif adalah metode representasi keseluruhan himpunan data spesifik dengan memberikan ringkasan pendek tentang sampel dan ukuran data Statistika Deskriptif juga merupakan metode yang sangat sederhana karena hanya mendeskripsikan kondisi dari data yang dimiliki dan menyajikannya dalam bentuk tabel diagram grafik dan bentuk lainnya yang di tampilkan dalam uraian singkat dan terbatas, sehingga dapat memberikan informasi yang berguna.","title":"Pengertian :\u00b6"},{"location":"StatistikaDeskriptif/#tipe-statistik-deskriptif","text":"","title":"Tipe Statistik Deskriptif :\u00b6"},{"location":"StatistikaDeskriptif/#mean","text":"Mean merupakan rata-rata dari keseluruhan angka. Mean didapatkan dari hasil penjumlahan dari keseluruhan angka yang dibagi dengan banyaknya angka itu sendiri. Untuk menghitung data, kita misalkan N data dengan rumus berikut :\u00afx=n\u2211i=1xiN=x1+x2+x3+...+xnNx\u00af=\u2211i=1nxiN=x1+x2+x3+...+xnNKeterangan : x bar = x rata-rata = nilai rata-rata sampel x = data ke n n = banyak data","title":"Mean\u00b6"},{"location":"StatistikaDeskriptif/#median","text":"Median merupakan nilai tengah dari sebuah urutan data, median disimbolkan dengan Me. Nilai dari median akan sama dengan nilai Quartil 2 / Q2. dalam mencari median yang banyak n dari data ganjil dan genap memiliki cara perhitungan data yang berbeda, dengan rumus sebagai berikut :Me=Q2=(n+12),jikanganjilMe=Q2=(n+12),jikanganjil Me=Q2=(xn2xn+122),jikangenapMe=Q2=(xn2xn+122),jikangenap Keterangan : Me = Nilai tengah dari kelompok data n = banyak data","title":"Median\u00b6"},{"location":"StatistikaDeskriptif/#modus","text":"Modus merupakan nilai / angka yang paling sering ditemukan dalam sebuah kelompok angka, atau data yang paling sering muncul, atau memiliki frekuensi tertinggi. Modus dilambangkan dengan Mo. dapat dihitung denganrumus berikut :Mo=Tb+pb1b1+b2Mo=Tb+pb1b1+b2keterangan : Mo = modus dari kelompok data Tb = tepi bawah dari elemen modus b1 = selisih frekuensi antara elemen modus dengan elemet sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen sesudahnya p = panjang interval nilai b1 dan b2 \u2013> adalah mutlak / selalu positif","title":"Modus\u00b6"},{"location":"StatistikaDeskriptif/#varians","text":"Varians adalah ukuran penyebaran setiap nilai dalam suatu himpunan data dari rata-rata. Dalam proses mencari varian terdapat langkah yang harus dilakukan, dengan mengambil ukuran jarak dari setiap nilai dan mengurangi rata-rata dari setiap nilai dalam data, kemudian hasil dari ukuran jarak tersebut dikuadratkan dan membagi jumlah kuadrat dengan jumlah nilai dalam himpunan data. Dapat dihitung dengan rumus berikut :\u03c32=n\u2211i=1(xi\u2212\u00afx)2n\u03c32=\u2211i=1n(xi\u2212x\u00af)2nKeterangan : Xi = titik data x bar = rata-rata dari semua titik data n = banyak dari dari anggota data","title":"Varians\u00b6"},{"location":"StatistikaDeskriptif/#standar-deviasi","text":"Standar Deviasi merupakan simpanan baku atau ukuran dispersi kumpulan data relatif terhadap rata-rata atau lebih simpelnya adalah akar kuadrat positif dari varian. Standar deviasi dihitung dengan mengakar kuadratkan nilai dari varians. jika titik data lebih dari rata-rata dalam kumpulan data maka semakin tinggi standar deviasi. dapat dihitung dengan rumus berikut :\u03c3=\ue001\ue000 \ue000 \ue000\u23b7n\u2211i=1(xi\u2212\u00afx)2n\u03c3=\u2211i=1n(xi\u2212x\u00af)2nKeterangan : xi = nilai x ke i x = rata rata n = ukuran sampel","title":"Standar Deviasi\u00b6"},{"location":"StatistikaDeskriptif/#skewness","text":"Skewness merupakan kemiringan atau ketidak simetrisan pada suatu distribusi statistik dimana kurva tampak condong ke kiri / ke kanan. Skewness digunakan untuk menentukan sejauh mana perbedaan suatu distribusi dengan distribusi normal. Dalam distribusi normal grafik muncul seperti kurva berbentuk lonceng. ketika suatu distribusi mengalami kemiringan ke sebelah kanan dan ekor di sisi kanan kurva lebih panjang dari ekor sisi kiri kurva maka situasi ini dikatakan kemiringan positif dan sebaliknya dikatakan kemiringan negative. Skewness dapat dihitung menggunakan rumus sebagai berikut:Skewness=\u2211i=1n(xi\u2212\u00afx)i(n\u22121)\u03c33Skewness=\u2211i=1n(xi\u2212x\u00af)i(n\u22121)\u03c33Keterangan : xi= titik data x bar = rata-rata dari distribusi n = jumlah titik dalam distribusi o = standar deviasi Quartile Quartile merupakan nilai-nilai yang membagi data yang telah diurutkan kedalam 4 bagian yang sama besar. Kuartil dinotasikan dengan notasi Q. Kuartil terdiri dari 3, yaitu kuartil pertama Q1, kuartil kedua Q2, dan kuartil ketiga Q3. Untuk menentukan kuartil pada data tunggal, kita harus mempertimbangkan banyaknya data n terlebih dahulu. Penghitungan quartil tergantung dari kondisi banyaknya data tersebut, Dalam mencari quatile kita dapat menggunakan rumus berikut ini:Q1=(n+1)14Q1=(n+1)14 Q2=(n+1)12Q2=(n+1)12 Q3=(n+1)34Q3=(n+1)34 Keterangan : Q = nilai quartil n = banyak data","title":"Skewness\u00b6"},{"location":"StatistikaDeskriptif/#penerapan-statistik-deskriptif-menggunakan-python","text":"","title":"Penerapan Statistik Deskriptif Menggunakan Python\u00b6"},{"location":"StatistikaDeskriptif/#alat-dan-bahan","text":"Pada penerapan ini saya menggunakan 100 data random yang disimpan dalam bentuk .csv dan untuk mempermudah dalam penerapan tersebut, perlu disiapkan library python yang dapat didownload secara gratis. library python yang digunakan adalah sebagai berikut : pandas, digunakan untuk data manajemen dan data analysis. scipy, merupakan library berisi kumpulan algoritma dan fungsi matematika.","title":"Alat dan Bahan :\u00b6"},{"location":"StatistikaDeskriptif/#langkah-langkah","text":"","title":"Langkah - Langkah\u00b6"},{"location":"StatistikaDeskriptif/#pertama","text":"Pertama-tama masukkan library yang sudah disiapkan from scipy import stats import pandas as pd","title":"PERTAMA\u00b6"},{"location":"StatistikaDeskriptif/#kedua","text":"Selanjutnya memuat data csv yang telah disiapkan df = pd.read_csv('data.csv') df setelah di run maka program akan menampilkan seperti berikut : tinggi badan berat badan tekanan darah lingkar perut 0 165 53 112 50 1 152 66 104 69 2 171 55 120 42 3 155 59 127 63 4 158 49 111 47 5 173 51 103 51 6 165 71 119 47 7 157 46 123 43 8 169 80 101 62 9 160 48 130 41 10 157 62 127 60 11 157 60 106 47 12 151 67 129 70 13 177 43 111 48 14 159 52 119 43 15 179 64 117 47 16 163 76 106 48 17 164 57 113 48 18 155 61 121 41 19 174 67 124 66 20 154 52 122 61 21 155 45 117 58 22 165 49 108 51 23 168 72 127 57 24 165 77 116 56 25 161 49 113 43 26 156 64 127 57 27 166 49 125 68 28 164 78 129 63 29 151 46 100 57 ... ... ... ... ... 70 170 73 107 42 71 157 70 105 63 72 180 44 100 54 73 162 49 116 62 74 171 62 103 41 75 169 42 126 59 76 165 51 111 66 77 157 61 109 56 78 163 79 101 65 79 151 58 119 60 80 161 54 129 40 81 177 72 106 63 82 179 46 117 40 83 162 78 106 41 84 172 69 116 60 85 167 67 115 44 86 162 54 117 55 87 170 72 128 59 88 159 78 124 44 89 178 43 105 48 90 177 79 111 67 91 165 62 106 70 92 170 56 119 66 93 150 59 117 63 94 163 65 115 58 95 164 76 129 45 96 160 55 129 49 97 157 40 124 49 98 156 47 123 61 99 162 65 126 67 100 rows \u00d7 4 columns","title":"KEDUA\u00b6"},{"location":"StatistikaDeskriptif/#ketiga","text":"Kemudian membuat data penyimpanan / dictionary yang menampung nilai yang ditampilkan. selanjutnya mengambil data dari beberapa kolom pada csv dengan cara diiterasi serta, menghitungnya dengan berbagai metode yang telah disiapkan oleh pandas itu sendiri. kemudian hasil tersebut di disimpan pada penyimpanan yang tadi. from IPython.display import HTML, display import tabulate table=[ [\"method\"]+[x for x in df.columns], [\"count()\"]+[df[col].count() for col in df.columns], [\"mean()\"]+[df[col].mean() for col in df.columns], [\"std()\"]+[\"{:.2f}\".format(df[col].std()) for col in df.columns], [\"min()\"]+[df[col].min() for col in df.columns], [\"max()\"]+[df[col].max() for col in df.columns], [\"q1()\"]+[df[col].quantile(0.25) for col in df.columns], [\"q2()\"]+[df[col].quantile(0.50) for col in df.columns], [\"q3\"]+[df[col].quantile(0.75) for col in df.columns], [\"skew\"]+[\"{:.2f}\".format(df[col].skew()) for col in df.columns], ] display(HTML(tabulate.tabulate(table, tablefmt='html')))","title":"KETIGA\u00b6"},{"location":"StatistikaDeskriptif/#keempat","text":"Yang terakhir yaitu memvisualisasikan hasil tersebut dalam bentuk data frame display(HTML(tabulate.tabulate(table, tablefmt='html'))) setelah di run maka program akan menampilkan seperti berikut : method tinggi badan berat badan tekanan darah lingkar perut count() 100 100 100 100 mean() 164.8 60.43 114.88 55.51 std() 7.86 11.81 8.81 9.59 min() 150 40 100 40 max() 180 80 130 70 q1() 158.0 49.75 106.75 47.0 q2() 165.0 60.5 115.5 57.0 q3 171.0 71.0 122.25 63.25 skew 0.07 0.06 -0.04 -0.15","title":"KEEMPAT\u00b6"},{"location":"Tugas 5 - Clustering/","text":"CLUSTERING \u00b6 Clustering ialah metode penganalisaan data yang sering dimasukkan sebagai salah satu metode Data Mining yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu wilayah yang sama dan data dengan karakteristik yang berbeda ke wilayah yang lain. \u200b Memilih Cluster yang terbaik dengan cara harus menghitung nilai silhotle coefisien atau silhotlenya mendekati nilai 1. \u200b Ada beberapa pendekatan yang digunakan dalam mengembangkan metode clustering, dua pendekatan utama adalah clustering dengan pendekatan partisi dan clustering dengan pendekatan hirarki. Clustering dengan pendekatan partisi atau sering disebut dengan partition-based clustering mengelompokkan data dengan memilah-milah data yang dianalisa ke dalam cluster-cluster yang ada. Clustering dengan pendekatan hirarki atau sering disebut dengan hierarchical clustering mengelompokkan data dengan membuat suatu hirarki berupa dendogram dimana data yang mirip akan ditempatkan pada hirarki yang berdekatan dan yang tidak pada hirarki yang berjauhan. Di samping kedua pendekatan tersebut, ada juga clustering dengan pendekatan automatic mapping (Self-Organising Map/SOM). Clustering dengan pendekatan partisi \u00b6 \u200b Metode yang bagus apabila dekat dengan pusat cluster dan jauh dengan pusat cluster yang lainnya. 1. K-Means \u200b Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi ini adalah metode k-means. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: \u00b7 Tentukan jumlah cluster \u00b7 Alokasikan data secara random ke cluster yang ada \u00b7 Hitung rata-rata setiap cluster dari data yang tergabung di dalamnya \u00b7 Alokasikan kembali semua data ke cluster terdekat \u00b7 Ulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold \u200b Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy . Setelah meneliti clustering dari sudut yang lain, saya menemukan bahwa k-means clustering mempunyai beberapa kelemahan. Fungsi dari algoritma ini adalah mengelompokkan data kedalam beberapa cluster. karakteristik dari algoritma ini adalah : . Memiliki n buah data. . Input berupa jumlah data dan jumlah cluster (kelompok). . Pada setiap cluster/kelompok memiliki sebuah centroid yang mempresentasikan cluster tersebut. Algoritma K-Means \u00b6 \u200b Secara sederhana algoritma K-Means dimulai dari tahap berikut : . Pilih K buah titik centroid. . Menghitung jarak data dengan centroid. . Update nilai titik centroid. . Ulangi langkah 2 dan 3 sampai nilai dari titik centroid tidak lagi berubah. Rumus K-Means \u00b6 Metode K-Modes \u00b6 \u200b K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster. Metode K-Prototype \u00b6 \u200b Tujuan dari simulasi ini adalah mencoba menerapkan algoritma K-Prototype pada data campuran numerik dan kategorikal. Ada tahap preparation diperlakukan terhadap data point numerik normalisasi terlebih dahulu. Algoritma K-Prototype \u00b6 \u200b Sebelum masuk proses algoritma K-Prototypes tentukan jumlah k yang akan dibentuk batasannya minimal 2 dan maksimal \u221an atau n/2 dimana n adalah jumlah data point atau obyek . Tahap 1 : Tentukan K dengan inisial kluster z1, z2, ..., zk secara acak dari n buah titik {x1, x2, ..., xn} . Tahap 2 : Hitung jarak seluruh data point pada data set terhadap inisial kluster awal, alokasikan data point ke dalam cluster yang memiliki jarak prototype terdekat dengan object yang diukur. . Tahap 3 : Hitung titik pusat cluster yang baru setelah semua objek dialokasikan. Lalu realokasikan semua datapoint pada dataset terhadap prototype yang baru. . Tahap 4 : jika titik pusat cluster tidak berubah atau sudah konvergen maka proses algoritma berhenti tetapi jika titik pusat masih berubah-ubah secara signifikan maka proses kembali ke tahap 2 dan 3 hingga iterasi maksimum tercapai atau sudah tidak ada perpindahan objek. Rumus K-Prototype \u00b6 2. Mixture Modelling (Mixture Modeling) \u200b Mixture modelling merupakan metode pengelompokan data yang mirip dengan k-means dengan kelebihan penggunaan distribusi statistik dalam mendefinisikan setiap cluster yang ditemukan. Dibandingkan dengan k-means yang hanya menggunakan cluster center, penggunaan distribusi statistik ini mengijinkan kita untuk: \u00b7 Memodel data yang kita miliki dengan setting karakteristik yang berbeda-beda \u00b7 Jumlah cluster yang sesuai dengan keadaan data bisa ditemukan seiring dengan proses pemodelan karakteristik dari masing-masing cluster \u00b7 Hasil pemodelan clustering yang dilaksanakan bisa diuji tingkat keakuratannya \u200b Distribusi statistik yang digunakan bisa bermacam-macam mulai dari yang digunakan untuk data categorical sampai yang continuous, termasuk di antaranya distribusi binomial, multinomial, normal dan lain-lain. Beberapa distribusi yang bersifat tidak normal seperti distribusi Poisson, von-Mises, Gamma dan Student t, juga diterapkan untuk bisa mengakomodasi berbagai keadaan data yang ada di lapangan. Beberapa pendekatan multivariate juga banyak diterapkan untuk memperhitungkan tingkat keterkaitan antara variabel data yang satu dengan yang lainnya. Clustering dengan Pendekatan Hirarki \u00b6 \u200b Clustering dengan pendekatan hirarki mengelompokkan data yang mirip dalam hirarki yang sama dan yang tidak mirip di hirarki yang agak jauh. Ada dua metode yang sering diterapkan yaitu agglomerative hieararchical clustering dan divisive hierarchical clustering . Agglomerative melakukan proses clustering dari N cluster menjadi satu kesatuan cluster, dimana N adalah jumlah data, sedangkan divisive melakukan proses clustering yang sebaliknya yaitu dari satu cluster menjadi N cluster. \u200b Beberapa metode hierarchical clustering yang sering digunakan dibedakan menurut cara mereka untuk menghitung tingkat kemiripan. Ada yang menggunakan Single Linkage , Complete Linkage , Average Linkage , Average Group Linkage dan lain-lainnya. Seperti juga halnya dengan partition-based clustering , kita juga bisa memilih jenis jarak yang digunakan untuk menghitung tingkat kemiripan antar data. \u200b Salah satu cara untuk mempermudah pengembangan dendogram untuk hierarchical clustering ini adalah dengan membuat similarity matrix yang memuat tingkat kemiripan antar data yang dikelompokkan. Tingkat kemiripan bisa dihitung dengan berbagai macam cara seperti dengan Euclidean Distance Space. Berangkat dari similarity matrix ini, kita bisa memilih lingkage jenis mana yang akan digunakan untuk mengelompokkan data yang dianalisa. Clustering Dengan Pendekatan Automatic Mapping (Self-Organising Map/SOM) \u00b6 \u200b Self-Organising Map merupakan suatu tipe Artificial Neural Networks yang di-training secara unsupervised. SOM menghasilkan map yang terdiri dari output dalam dimensi yang rendah (2 atau 3 dimensi). Map ini berusaha mencari property dari input data. Komposisi input dan output dalam SOM mirip dengan komposisi dari proses feature scaling (multidimensional scaling). \u200b Walaupun proses learning yang dilakukan mirip dengan Artificial Neural Networks, tetapi proses untuk meng-assign input data ke map, lebih mirip dengan K-Means dan kNN Algorithm. Adapun prosedur yang ditempuh dalam melakukan clustering dengan SOM adalah sebagai berikut: \u00b7 Tentukan weight dari input data secara random \u00b7 Pilih salah satu input data \u00b7 Hitung tingkat kesamaan (dengan Eucledian) antara input data dan weight dari input data tersebut dan pilih input data yang memiliki kesamaan dengan weight yang ada (data ini disebut dengan Best Matching Unit (BMU)) \u00b7 Perbaharui weight dari input data dengan mendekatkan weight tersebut ke BMU dengan rumus: Wv(t+1) = Wv(t) + Theta(v, t) x Alpha(t) x (D(t) \u2013 Wv(t)) Dimana: o Wv(t) : Weight pada saat ke-t o Theta (v, t) : Fungsi neighbourhood yang tergantung pada Lattice distance antara BMU dengan neuron v. Umumnya bernilai 1 untuk neuron yang cukup dekat dengan BMU, dan 0 untuk yang sebaliknya. Penggunaan fungsi Gaussian juga memungkinkan. o Alpha (t) : Learning Coefficient yang berkurang secara monotonic o D(t) : Input data \u00b7 Tambah nilai t, sampai t < Lambda , dimana Lambda adalah jumlah iterasi Contoh clustering: \u00b6 Data type categorical ID X1 X2 1 4 6 2 7 5 3 2 2 4 1 3 5 2 1 misal pusatnya di 1 dan 3, mencari nilai d satu-persatu maka nilai 4 ada di kelompok yang ada nilai 3 jadi pada saat iterasi I kelompok 1 (1,2) kelompok 2 (3,4,5) pada saat iterasi II kelompok 1 (3,4,5) kelompok 2 (1,2) MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Clustering"},{"location":"Tugas 5 - Clustering/#clustering","text":"Clustering ialah metode penganalisaan data yang sering dimasukkan sebagai salah satu metode Data Mining yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu wilayah yang sama dan data dengan karakteristik yang berbeda ke wilayah yang lain. \u200b Memilih Cluster yang terbaik dengan cara harus menghitung nilai silhotle coefisien atau silhotlenya mendekati nilai 1. \u200b Ada beberapa pendekatan yang digunakan dalam mengembangkan metode clustering, dua pendekatan utama adalah clustering dengan pendekatan partisi dan clustering dengan pendekatan hirarki. Clustering dengan pendekatan partisi atau sering disebut dengan partition-based clustering mengelompokkan data dengan memilah-milah data yang dianalisa ke dalam cluster-cluster yang ada. Clustering dengan pendekatan hirarki atau sering disebut dengan hierarchical clustering mengelompokkan data dengan membuat suatu hirarki berupa dendogram dimana data yang mirip akan ditempatkan pada hirarki yang berdekatan dan yang tidak pada hirarki yang berjauhan. Di samping kedua pendekatan tersebut, ada juga clustering dengan pendekatan automatic mapping (Self-Organising Map/SOM).","title":"CLUSTERING"},{"location":"Tugas 5 - Clustering/#clustering-dengan-pendekatan-partisi","text":"\u200b Metode yang bagus apabila dekat dengan pusat cluster dan jauh dengan pusat cluster yang lainnya. 1. K-Means \u200b Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi ini adalah metode k-means. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: \u00b7 Tentukan jumlah cluster \u00b7 Alokasikan data secara random ke cluster yang ada \u00b7 Hitung rata-rata setiap cluster dari data yang tergabung di dalamnya \u00b7 Alokasikan kembali semua data ke cluster terdekat \u00b7 Ulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold \u200b Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy . Setelah meneliti clustering dari sudut yang lain, saya menemukan bahwa k-means clustering mempunyai beberapa kelemahan. Fungsi dari algoritma ini adalah mengelompokkan data kedalam beberapa cluster. karakteristik dari algoritma ini adalah : . Memiliki n buah data. . Input berupa jumlah data dan jumlah cluster (kelompok). . Pada setiap cluster/kelompok memiliki sebuah centroid yang mempresentasikan cluster tersebut.","title":"Clustering dengan pendekatan partisi"},{"location":"Tugas 5 - Clustering/#algoritma-k-means","text":"\u200b Secara sederhana algoritma K-Means dimulai dari tahap berikut : . Pilih K buah titik centroid. . Menghitung jarak data dengan centroid. . Update nilai titik centroid. . Ulangi langkah 2 dan 3 sampai nilai dari titik centroid tidak lagi berubah.","title":"Algoritma K-Means"},{"location":"Tugas 5 - Clustering/#rumus-k-means","text":"","title":"Rumus K-Means"},{"location":"Tugas 5 - Clustering/#metode-k-modes","text":"\u200b K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster.","title":"Metode K-Modes"},{"location":"Tugas 5 - Clustering/#metode-k-prototype","text":"\u200b Tujuan dari simulasi ini adalah mencoba menerapkan algoritma K-Prototype pada data campuran numerik dan kategorikal. Ada tahap preparation diperlakukan terhadap data point numerik normalisasi terlebih dahulu.","title":"Metode K-Prototype"},{"location":"Tugas 5 - Clustering/#algoritma-k-prototype","text":"\u200b Sebelum masuk proses algoritma K-Prototypes tentukan jumlah k yang akan dibentuk batasannya minimal 2 dan maksimal \u221an atau n/2 dimana n adalah jumlah data point atau obyek . Tahap 1 : Tentukan K dengan inisial kluster z1, z2, ..., zk secara acak dari n buah titik {x1, x2, ..., xn} . Tahap 2 : Hitung jarak seluruh data point pada data set terhadap inisial kluster awal, alokasikan data point ke dalam cluster yang memiliki jarak prototype terdekat dengan object yang diukur. . Tahap 3 : Hitung titik pusat cluster yang baru setelah semua objek dialokasikan. Lalu realokasikan semua datapoint pada dataset terhadap prototype yang baru. . Tahap 4 : jika titik pusat cluster tidak berubah atau sudah konvergen maka proses algoritma berhenti tetapi jika titik pusat masih berubah-ubah secara signifikan maka proses kembali ke tahap 2 dan 3 hingga iterasi maksimum tercapai atau sudah tidak ada perpindahan objek.","title":"Algoritma K-Prototype"},{"location":"Tugas 5 - Clustering/#rumus-k-prototype","text":"2. Mixture Modelling (Mixture Modeling) \u200b Mixture modelling merupakan metode pengelompokan data yang mirip dengan k-means dengan kelebihan penggunaan distribusi statistik dalam mendefinisikan setiap cluster yang ditemukan. Dibandingkan dengan k-means yang hanya menggunakan cluster center, penggunaan distribusi statistik ini mengijinkan kita untuk: \u00b7 Memodel data yang kita miliki dengan setting karakteristik yang berbeda-beda \u00b7 Jumlah cluster yang sesuai dengan keadaan data bisa ditemukan seiring dengan proses pemodelan karakteristik dari masing-masing cluster \u00b7 Hasil pemodelan clustering yang dilaksanakan bisa diuji tingkat keakuratannya \u200b Distribusi statistik yang digunakan bisa bermacam-macam mulai dari yang digunakan untuk data categorical sampai yang continuous, termasuk di antaranya distribusi binomial, multinomial, normal dan lain-lain. Beberapa distribusi yang bersifat tidak normal seperti distribusi Poisson, von-Mises, Gamma dan Student t, juga diterapkan untuk bisa mengakomodasi berbagai keadaan data yang ada di lapangan. Beberapa pendekatan multivariate juga banyak diterapkan untuk memperhitungkan tingkat keterkaitan antara variabel data yang satu dengan yang lainnya.","title":"Rumus K-Prototype"},{"location":"Tugas 5 - Clustering/#clustering-dengan-pendekatan-hirarki","text":"\u200b Clustering dengan pendekatan hirarki mengelompokkan data yang mirip dalam hirarki yang sama dan yang tidak mirip di hirarki yang agak jauh. Ada dua metode yang sering diterapkan yaitu agglomerative hieararchical clustering dan divisive hierarchical clustering . Agglomerative melakukan proses clustering dari N cluster menjadi satu kesatuan cluster, dimana N adalah jumlah data, sedangkan divisive melakukan proses clustering yang sebaliknya yaitu dari satu cluster menjadi N cluster. \u200b Beberapa metode hierarchical clustering yang sering digunakan dibedakan menurut cara mereka untuk menghitung tingkat kemiripan. Ada yang menggunakan Single Linkage , Complete Linkage , Average Linkage , Average Group Linkage dan lain-lainnya. Seperti juga halnya dengan partition-based clustering , kita juga bisa memilih jenis jarak yang digunakan untuk menghitung tingkat kemiripan antar data. \u200b Salah satu cara untuk mempermudah pengembangan dendogram untuk hierarchical clustering ini adalah dengan membuat similarity matrix yang memuat tingkat kemiripan antar data yang dikelompokkan. Tingkat kemiripan bisa dihitung dengan berbagai macam cara seperti dengan Euclidean Distance Space. Berangkat dari similarity matrix ini, kita bisa memilih lingkage jenis mana yang akan digunakan untuk mengelompokkan data yang dianalisa.","title":"Clustering dengan Pendekatan Hirarki"},{"location":"Tugas 5 - Clustering/#clustering-dengan-pendekatan-automatic-mapping-self-organising-mapsom","text":"\u200b Self-Organising Map merupakan suatu tipe Artificial Neural Networks yang di-training secara unsupervised. SOM menghasilkan map yang terdiri dari output dalam dimensi yang rendah (2 atau 3 dimensi). Map ini berusaha mencari property dari input data. Komposisi input dan output dalam SOM mirip dengan komposisi dari proses feature scaling (multidimensional scaling). \u200b Walaupun proses learning yang dilakukan mirip dengan Artificial Neural Networks, tetapi proses untuk meng-assign input data ke map, lebih mirip dengan K-Means dan kNN Algorithm. Adapun prosedur yang ditempuh dalam melakukan clustering dengan SOM adalah sebagai berikut: \u00b7 Tentukan weight dari input data secara random \u00b7 Pilih salah satu input data \u00b7 Hitung tingkat kesamaan (dengan Eucledian) antara input data dan weight dari input data tersebut dan pilih input data yang memiliki kesamaan dengan weight yang ada (data ini disebut dengan Best Matching Unit (BMU)) \u00b7 Perbaharui weight dari input data dengan mendekatkan weight tersebut ke BMU dengan rumus: Wv(t+1) = Wv(t) + Theta(v, t) x Alpha(t) x (D(t) \u2013 Wv(t)) Dimana: o Wv(t) : Weight pada saat ke-t o Theta (v, t) : Fungsi neighbourhood yang tergantung pada Lattice distance antara BMU dengan neuron v. Umumnya bernilai 1 untuk neuron yang cukup dekat dengan BMU, dan 0 untuk yang sebaliknya. Penggunaan fungsi Gaussian juga memungkinkan. o Alpha (t) : Learning Coefficient yang berkurang secara monotonic o D(t) : Input data \u00b7 Tambah nilai t, sampai t < Lambda , dimana Lambda adalah jumlah iterasi","title":"Clustering Dengan Pendekatan Automatic Mapping (Self-Organising Map/SOM)"},{"location":"Tugas 5 - Clustering/#contoh-clustering","text":"Data type categorical ID X1 X2 1 4 6 2 7 5 3 2 2 4 1 3 5 2 1 misal pusatnya di 1 dan 3, mencari nilai d satu-persatu maka nilai 4 ada di kelompok yang ada nilai 3 jadi pada saat iterasi I kelompok 1 (1,2) kelompok 2 (3,4,5) pada saat iterasi II kelompok 1 (3,4,5) kelompok 2 (1,2) MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Contoh clustering:"},{"location":"index2/","text":"Komputasi Numerik \u00b6 Nama : Mareta Kurnia Sari NIM : 180411100090 Alamat : Sumenep Kelas : Komputasi Numerik 4B Jurusan : Teknik Informatika Angkatan : 2018 Dosen Pengampu : Mula'ab,S.Si.,M.Kom","title":"home"},{"location":"index2/#komputasi-numerik","text":"Nama : Mareta Kurnia Sari NIM : 180411100090 Alamat : Sumenep Kelas : Komputasi Numerik 4B Jurusan : Teknik Informatika Angkatan : 2018 Dosen Pengampu : Mula'ab,S.Si.,M.Kom","title":"Komputasi Numerik"},{"location":"regresiLinier/","text":"Regresi Linier Sederhana dan Berganda \u00b6 \u200b Dalam berbagai penulisan laporan penelitian ilmiah, analisis regresi banyak sekali digunakan. Bahkan, bisa jadi, analisis ini paling banyak digunakan. Analisis regresi menunjukkan pengaruh variabel independen atau variabel bebas (X) terhadap variabel dependen atau variabel tergantung (Y). Dengan demikian, setidaknya ada 2 variabel yang terlibat dalam uji atau analisis regresi yaitu 1). variabel independen atau variabel bebas (X), dan 2) variabel dependen atau variabel tergantung (Y). \u200b Analisis regresi terbagi menjadi dua yaitu regresi linier dan Nonlinier. Analisi regresi linear terdiri dari analisis regresi linear sederhana dan analisis regresi linear berganda. Perbedaan antar keduanya terletak pada jumlah variabel independennya. Regresi linear sederhana hanya memiliki satu variabel independen, sedangkan regresi linear berganda mempunyai banyak variabel independen. Analisis regresi Nonlinier adalah regresi eksponensial. Regresi Linier Sederhana \u00b6 Regresi linier sederhana adalah regresi yang hanya melibatkan dua variabel, yaitu 1 (satu) variabel dependen atau variabel tergantung dan 1 (satu) variabel independen atau bebas. Persamaan di atas adalah rumus dari persamaan regresi linear sederhana. Y adalah variabel tak bebas, a adalah koefisien intersep, b adalah kemiringan dan t adalah variabel bebas. Rumus untuk b adalah : Dan rumus untuk mendapatkan nilai a adalah sebagai berikut : Dalam regresi linear sederhana juga ada yang disebut dengan koefisien korelasi yang menunjukkan bahwa nilai suatu variabel bergantung pada perubahan nilai variabel yang lain. Rumus untuk menghitung koefisien korelasi adalah sebagai berikut : Regresi Linier Berganda \u00b6 Regresi berganda adalah model regresi atau prediksi yang melibatkan lebih dari satu variabel bebas atau prediktor. Istilah regresi berganda dapat disebut juga dengan istilah multiple regression. Kata multiple berarti jamak atau lebih dari satu variabel. Persamaan regresi linear berganda sebagai berikut: Y\u2019 = a + b1X1+ b2X2+\u2026..+ bnXn Keterangan: Y\u2019 = Variabel dependen (nilai yang diprediksikan) X1 dan X2 = Variabel independen a = Konstanta (nilai Y\u2019 apabila X1, X2\u2026..Xn = 0) b = Koefisien regresi (nilai peningkatan ataupun penurunan) Contoh kasus dengan penghitungan manual \u00b6 Sebuah penelitian terhadap pohon Mahoni, dimana akan diteliti apakah ada hubungan antara tinggi pohon dengan diameter batang pohon, dengan artian apakah ada pengaruh diameter batang pohon terhadap tinggi pohon tersebut. Diambil sampel secara acak sejumlah delapan pohon mahoni.Dapat dilihat dari Tabel 1 pada kolom X dan Y. Hal pertama yang akan kita lakukan adalah membentuk persamaan regresi, yaitu : Y' = a + bX Selanjutnya adalah menentukan konstanta a dan koefisien b, kita ikuti langkah sebagai berikut : tinggi pohon diameter batang xy y2 x2 y x 35 8 280 1225 64 49 9 441 2401 81 27 7 189 729 49 33 6 198 1089 36 60 13 780 3600 169 21 7 147 441 49 45 11 495 2025 121 51 12 612 2601 144 321 73 3142 14111 713 maka diperoleh : Persamaan regresi diperoleh : Y' = -1,3147 + 4,5413X dimana : Y' = Tinggi pohon mahoni yang diprediksi X = Diameter batang pohon mahoni Interpretasi dari koefisien regresi : Nilai a = -1,3147 artinya tidak ada diameter batang pohon maka tidak ada tinggi pohon. (karena tidak ada tinggi yang bernilai negatif sehingga dianggap nol). Nilai b = 4,5413 artinya jika terjadi peningkatan diameter batang pohon mahoni satu satuan maka akan terjadi peningkatan tinggi pohon mahoni sebesar 4,5413 satuan. Koefisien Determinasi R2 : r = 0,886 bernilai positif dan kuat artinya terdapat hubungan atau korelasi yang kuat antara tinggi pohon mahoni dengan diameter batang pohon mahoni. Semakin besar diameter batang pohon mahoni maka semakin tinggi batang pohon mahoni. R2 = 0,8862 = 0,785 artinya sekitar 78,5% variasi dari variabel diameter batang pohon mahoni dapat menjelaskan variasi dari variabel tinggi pohon mahoni. (cukup tinggi) Standar Error Estimate Persamaan Regresi: Jadi besarnya standar error estimate persamaan regresi adalah 6,6364. Hal ini menunjukkan penyimpangan data-data terhadap garis regresi, atau bagaimana penyimpangan data yang menyebar disekitar garis regresi. (cukup kecil). Pengujian Koefisien Regresi : > Hipotesis Uji Ho : b = 0 Ha : b \u2260 0 > Taraf Signifikansi Pilih nilai signifikansi a = 5% > Daerah Kritis dengan nilai a = 5% dan derajat bebas n-2=8-2=6, maka diperoleh nilai t-tabel pada 5%/2 = 2,5% yaitu 2,447. > Statistik Uji > Keputusan nilai t-hitung = 4,6805 > t-tabel = 2,447 sehingga Ho ditolak dan Ha diterima. > Kesimpulan Dengan tingkat signifikansi 5% cukup menjelaskan bahwa ada pengaruh diameter batang pohon mahoni terhadap tinggi pohon mahoni. Contoh kasus dengan penghitungan sklearn \u00b6 Dalam pembelajaran kali ini, kita ingin mencari solusi dari proses perekrutan sebuah perusahaan. Perusahaan ini sedang merekrut seorang calon pegawai baru. Namun, bagian HRD perusahaan ini kebingungan, berapa gaji yang harus ia berikan, sesuai dengan level di mana calon pegawai baru ini masuk. Tentunya akan ada proses negosiasi antara HRD dengan calon pegawai baru ini tentang jumlah gaji yang pantas diterima pegawai tersebut. Calon pegawai ini mengaku bahwa sebelumnya ia telah berada di posisi Region Manager dengan pengalaman bekerja 20 tahun lebih dengan gaji hampir 160K dollar per tahun. Ia meminta perusahaan baru ini untuk memberikan ia gaji lebih dari 160K dollar per tahun. Untuk menyelidiki apakah calon pegawai ini benar-benar digaji sebanyak 160K dollar/tahun, maka bagian HRD membandingkan data gaji perusahaan tempat calon pegawai ini bekerja sebelumnya (kebetulan perusahaan memiliki daftar gajinya) dengan pengakuannya. Data yang dimiliki adalah daftar antara gaji dan level di perusahaan tersebut. Bagian HRD ingin mencari hubungan antara gaji yang didapat dengan level (tingkatan jabatan) di perusahaan calon pekerja tadi bekerja sebelumnya. Hasil penelitian awal, calon pegawai ini layak masuk di level 6.5 (antara region manager dan partner ). Berikut variabel yang kita miliki: Variabel dependen : Gaji (dalam dollar per tahun) Variabel independen : level (tingkatan jabatan) Setelah melihat tabelnya, bisa dilihat bahwa kita memiliki 1 variabel dependen, dan 1 variabel independen. Dari sini kita bisa tahu bahwa kita bisa menggunakan pendekatan model regresi sederhana. Walau demikian, datanya sudah diatur sedemikian rupa sehingga fungsi yang dimiliki antara variabel dependen dengan independen adalah kuadratik. Kita tetap akan mencoba membuat 2 model (simple dan polinomial) untuk membandingkan performanya (seberapa fit antara 2 model regresi ini dengan data). # Mengimpor library import`` numpy as np import`` matplotlib.pyplot as plt import`` pandas as pd Line 2 sampai line 4 mengimpor library yang diperlukan # Mengimpor dataset dataset ``=`` pd.read_csv(``'Posisi_gaji.csv'``) X ``=`` dataset.iloc[:, ``1``:``2``].values y ``=`` dataset.iloc[:, ``2``].values Line 7 mengimpor datasetnya Line 8 menentukan variabel independen X. Penting, bahwa usahakan variabel independen adalah matrix, dan bukan vector. Kita bisa saja menuliskan X = dataset.iloc[:, 1].values , namun perintah ini akan menghasilkan vector. Biasakan membuatnya sebagai sebuah matrix, dengan cara melakukan slicing X = dataset.iloc[:, 1:2].values . Bagaimana kita tahu X sudah menjadi matrix? Bisa dilihat kolom size di spyder variabel X adalah (10,1). Artinya X adalah matrix 10\u00d71 (10 baris dan 1 kolom). Line 9 menentukan variabel dependen y. Penting, usahakan variabel dependen adalah vector. Vektor ( vector ) adalah matriks yang hanya terdiri dari 1 kolom, atau matriks 1 baris. Cara membuatnya menjadi vektor adalah jangan lakukan slicing pada bagian kolomnya. Pada bagian size variabel y di spyder adalah (10,) yang artinya ia adalah matrix 1 baris. # Fitting Linear Regression ke dataset from`` sklearn.linear_model ``import`` LinearRegression lin_reg ``=`` LinearRegression() lin_reg.fit(X, y) Line 12 mengimpor class LinearRegression (untuk membuat model regresi sederhana) Line 13 mempersiapkan objek lin_reg sebagai model regresi sederhana Line 14 membuat model regresi sederhana (Kali ini tanpa membagi dataset ke dalam test dan train set, karena datasetnya terlalu kecil (biasanya train set minimal butuh 10 baris, dan kali ini tidak cukup data untuk dimasukkan ke test set). Walau demikian, model yang jadi nanti akan merupakan bagian dari train set, dan dataset baru yang diterima (pengujian train set) akan menjadi test set-nya). # Fitting Polynomial Regression ke dataset from`` sklearn.preprocessing ``import`` PolynomialFeatures poly_reg ``=`` PolynomialFeatures(degree ``=`` ``2``) ``## nantinya degree diganti menjadi 4 X_poly ``=`` poly_reg.fit_transform(X) lin_reg_2 ``=`` LinearRegression() lin_reg_2.fit(X_poly, y) Line 17 mengimpor PolynomialFeatures dari library sklearn.preprocessing untuk membuat model polinomial. Untuk mengetahui parameter apa saja yang diperlukan, cukup arahkan kursor pada PolynomialFeatures, lalu klik CTRL+i. Line 18 mempersiapkan objek poly_reg sebagai transformasi matriks X menjadi matriks X pangkat 2, pangkat 3 hingga pangkat n. Jadi nantinya kita memiliki beberapa tambahan variabel independen sebanyak n. Parameter default untuk PolynomialFeatures adalah degrees=2. Line 19 menyiapkan objek X_poly sebagai hasil fit_transform (proses fit dan transform dilakukan sekaligus) dari variabel X. Mari kita bandingkan antara X dengan X_poly. Line 20 menyiapkan objek lin_reg_2 sebagai model regresi polinomial. Line 21 membuat model regresi polinomial dengan parameter variabel independen adalah X_poly, dan variabel dependennya adalah y. # Visualisasi hasil regresi sederhana plt.scatter(X, y, color ``=`` ``'red'``) plt.plot(X, lin_reg.predict(X), color ``=`` ``'blue'``) plt.title(``'Sesuai atau tidak (Linear Regression)'``) plt.xlabel(``'Level posisi'``) plt.ylabel(``'Gaji'``) plt.show() Line 24 sampai line 29 adalah perintah untuk visualisasi hasil model regresi sederhana kita. Ingat untuk visualisasi, perintah dari line 24-29 harus dieksekusi bersamaan. Visualisasinya akan nampak sebagai berikut : # Visualisasi hasil regresi polynomial plt.scatter(X, y, color ``=`` ``'red'``) plt.plot(X, lin_reg_2.predict(X_poly), color ``=`` ``'blue'``) plt.title(``'Sesuai atau tidak (Polynomial Regression)'``) plt.xlabel(``'Level posisi'``) plt.ylabel(``'Gaji'``) plt.show() Line 32 sampai line 37 adalah perintah untuk visualisasi hasil model regresi polinomial. Pelru diingat sumbu y nya adalah lin_reg_2.predict(X_poly) . Hasilnya akan tampak sebagai berikut : Bisa dilihat dengan menggunakan fungsi polinomial hasilnya cukup baik. Namun tetap saja masih kurang cukup fit , di mana masih ada jarak antara model dengan data. Solusinya adalah pada line 18 kita ubah degree nya dari 2 menjadi 4. Eksekusi line 18 sampai line 21. Kemudian eksekusi line 32 sampai line 37. Maka visualisasi yang baru akan tampak sebagai berikut : # Memprediksi hasil dengan regresi sederhana lin_reg.predict(``6.5``) Line 40 adalah perintah untuk melihat dengan model regresi sederhana yang sudah dibuat, berapa gaji yang layak untuk tingkat level 6.5? Maka cukup ganti parameter X di lin_reg.predict(X) dengan angka 6.5. Jika dieksekusi, hasilnya adalah 330378.78 dollar/tahun. Tentunya prediksi dari regresi sederhana terlalu tinggi (terlihat juga di plot visualisasinya). Kita tidak menginginkan gaji yang terlalu tinggi yang merupakan hasil dari model regresi sederhana yang buruk kali ini. # Memprediksi hasil dengan regresi polynomial lin_reg_2.predict(poly_reg.fit_transform(``6.5``)) Line 43 adalah perintah untuk melihat prediksi gaji dengan model regresi polinomial. Perlu diperhatikan bahwa parameter X diganti dengan poly_reg.fit_transform(6.5) dan bukan X_poly. Karena kita ingin mengisi angka 6.5 sebagai parameter X. Sementara X_poky adalah hasil dari definisi fungsi poly_reg.fit_transform(X). Ketika dieksekusi maka hasilnya adalah 158862.45 dollar/tahun. Prediksi yang cukup baik, dengan model yang fit. MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Regresi Linier"},{"location":"regresiLinier/#regresi-linier-sederhana-dan-berganda","text":"\u200b Dalam berbagai penulisan laporan penelitian ilmiah, analisis regresi banyak sekali digunakan. Bahkan, bisa jadi, analisis ini paling banyak digunakan. Analisis regresi menunjukkan pengaruh variabel independen atau variabel bebas (X) terhadap variabel dependen atau variabel tergantung (Y). Dengan demikian, setidaknya ada 2 variabel yang terlibat dalam uji atau analisis regresi yaitu 1). variabel independen atau variabel bebas (X), dan 2) variabel dependen atau variabel tergantung (Y). \u200b Analisis regresi terbagi menjadi dua yaitu regresi linier dan Nonlinier. Analisi regresi linear terdiri dari analisis regresi linear sederhana dan analisis regresi linear berganda. Perbedaan antar keduanya terletak pada jumlah variabel independennya. Regresi linear sederhana hanya memiliki satu variabel independen, sedangkan regresi linear berganda mempunyai banyak variabel independen. Analisis regresi Nonlinier adalah regresi eksponensial.","title":"Regresi Linier Sederhana dan Berganda"},{"location":"regresiLinier/#regresi-linier-sederhana","text":"Regresi linier sederhana adalah regresi yang hanya melibatkan dua variabel, yaitu 1 (satu) variabel dependen atau variabel tergantung dan 1 (satu) variabel independen atau bebas. Persamaan di atas adalah rumus dari persamaan regresi linear sederhana. Y adalah variabel tak bebas, a adalah koefisien intersep, b adalah kemiringan dan t adalah variabel bebas. Rumus untuk b adalah : Dan rumus untuk mendapatkan nilai a adalah sebagai berikut : Dalam regresi linear sederhana juga ada yang disebut dengan koefisien korelasi yang menunjukkan bahwa nilai suatu variabel bergantung pada perubahan nilai variabel yang lain. Rumus untuk menghitung koefisien korelasi adalah sebagai berikut :","title":"Regresi Linier Sederhana"},{"location":"regresiLinier/#regresi-linier-berganda","text":"Regresi berganda adalah model regresi atau prediksi yang melibatkan lebih dari satu variabel bebas atau prediktor. Istilah regresi berganda dapat disebut juga dengan istilah multiple regression. Kata multiple berarti jamak atau lebih dari satu variabel. Persamaan regresi linear berganda sebagai berikut: Y\u2019 = a + b1X1+ b2X2+\u2026..+ bnXn Keterangan: Y\u2019 = Variabel dependen (nilai yang diprediksikan) X1 dan X2 = Variabel independen a = Konstanta (nilai Y\u2019 apabila X1, X2\u2026..Xn = 0) b = Koefisien regresi (nilai peningkatan ataupun penurunan)","title":"Regresi Linier Berganda"},{"location":"regresiLinier/#contoh-kasus-dengan-penghitungan-manual","text":"Sebuah penelitian terhadap pohon Mahoni, dimana akan diteliti apakah ada hubungan antara tinggi pohon dengan diameter batang pohon, dengan artian apakah ada pengaruh diameter batang pohon terhadap tinggi pohon tersebut. Diambil sampel secara acak sejumlah delapan pohon mahoni.Dapat dilihat dari Tabel 1 pada kolom X dan Y. Hal pertama yang akan kita lakukan adalah membentuk persamaan regresi, yaitu : Y' = a + bX Selanjutnya adalah menentukan konstanta a dan koefisien b, kita ikuti langkah sebagai berikut : tinggi pohon diameter batang xy y2 x2 y x 35 8 280 1225 64 49 9 441 2401 81 27 7 189 729 49 33 6 198 1089 36 60 13 780 3600 169 21 7 147 441 49 45 11 495 2025 121 51 12 612 2601 144 321 73 3142 14111 713 maka diperoleh : Persamaan regresi diperoleh : Y' = -1,3147 + 4,5413X dimana : Y' = Tinggi pohon mahoni yang diprediksi X = Diameter batang pohon mahoni Interpretasi dari koefisien regresi : Nilai a = -1,3147 artinya tidak ada diameter batang pohon maka tidak ada tinggi pohon. (karena tidak ada tinggi yang bernilai negatif sehingga dianggap nol). Nilai b = 4,5413 artinya jika terjadi peningkatan diameter batang pohon mahoni satu satuan maka akan terjadi peningkatan tinggi pohon mahoni sebesar 4,5413 satuan. Koefisien Determinasi R2 : r = 0,886 bernilai positif dan kuat artinya terdapat hubungan atau korelasi yang kuat antara tinggi pohon mahoni dengan diameter batang pohon mahoni. Semakin besar diameter batang pohon mahoni maka semakin tinggi batang pohon mahoni. R2 = 0,8862 = 0,785 artinya sekitar 78,5% variasi dari variabel diameter batang pohon mahoni dapat menjelaskan variasi dari variabel tinggi pohon mahoni. (cukup tinggi) Standar Error Estimate Persamaan Regresi: Jadi besarnya standar error estimate persamaan regresi adalah 6,6364. Hal ini menunjukkan penyimpangan data-data terhadap garis regresi, atau bagaimana penyimpangan data yang menyebar disekitar garis regresi. (cukup kecil). Pengujian Koefisien Regresi : > Hipotesis Uji Ho : b = 0 Ha : b \u2260 0 > Taraf Signifikansi Pilih nilai signifikansi a = 5% > Daerah Kritis dengan nilai a = 5% dan derajat bebas n-2=8-2=6, maka diperoleh nilai t-tabel pada 5%/2 = 2,5% yaitu 2,447. > Statistik Uji > Keputusan nilai t-hitung = 4,6805 > t-tabel = 2,447 sehingga Ho ditolak dan Ha diterima. > Kesimpulan Dengan tingkat signifikansi 5% cukup menjelaskan bahwa ada pengaruh diameter batang pohon mahoni terhadap tinggi pohon mahoni.","title":"Contoh kasus dengan penghitungan manual"},{"location":"regresiLinier/#contoh-kasus-dengan-penghitungan-sklearn","text":"Dalam pembelajaran kali ini, kita ingin mencari solusi dari proses perekrutan sebuah perusahaan. Perusahaan ini sedang merekrut seorang calon pegawai baru. Namun, bagian HRD perusahaan ini kebingungan, berapa gaji yang harus ia berikan, sesuai dengan level di mana calon pegawai baru ini masuk. Tentunya akan ada proses negosiasi antara HRD dengan calon pegawai baru ini tentang jumlah gaji yang pantas diterima pegawai tersebut. Calon pegawai ini mengaku bahwa sebelumnya ia telah berada di posisi Region Manager dengan pengalaman bekerja 20 tahun lebih dengan gaji hampir 160K dollar per tahun. Ia meminta perusahaan baru ini untuk memberikan ia gaji lebih dari 160K dollar per tahun. Untuk menyelidiki apakah calon pegawai ini benar-benar digaji sebanyak 160K dollar/tahun, maka bagian HRD membandingkan data gaji perusahaan tempat calon pegawai ini bekerja sebelumnya (kebetulan perusahaan memiliki daftar gajinya) dengan pengakuannya. Data yang dimiliki adalah daftar antara gaji dan level di perusahaan tersebut. Bagian HRD ingin mencari hubungan antara gaji yang didapat dengan level (tingkatan jabatan) di perusahaan calon pekerja tadi bekerja sebelumnya. Hasil penelitian awal, calon pegawai ini layak masuk di level 6.5 (antara region manager dan partner ). Berikut variabel yang kita miliki: Variabel dependen : Gaji (dalam dollar per tahun) Variabel independen : level (tingkatan jabatan) Setelah melihat tabelnya, bisa dilihat bahwa kita memiliki 1 variabel dependen, dan 1 variabel independen. Dari sini kita bisa tahu bahwa kita bisa menggunakan pendekatan model regresi sederhana. Walau demikian, datanya sudah diatur sedemikian rupa sehingga fungsi yang dimiliki antara variabel dependen dengan independen adalah kuadratik. Kita tetap akan mencoba membuat 2 model (simple dan polinomial) untuk membandingkan performanya (seberapa fit antara 2 model regresi ini dengan data). # Mengimpor library import`` numpy as np import`` matplotlib.pyplot as plt import`` pandas as pd Line 2 sampai line 4 mengimpor library yang diperlukan # Mengimpor dataset dataset ``=`` pd.read_csv(``'Posisi_gaji.csv'``) X ``=`` dataset.iloc[:, ``1``:``2``].values y ``=`` dataset.iloc[:, ``2``].values Line 7 mengimpor datasetnya Line 8 menentukan variabel independen X. Penting, bahwa usahakan variabel independen adalah matrix, dan bukan vector. Kita bisa saja menuliskan X = dataset.iloc[:, 1].values , namun perintah ini akan menghasilkan vector. Biasakan membuatnya sebagai sebuah matrix, dengan cara melakukan slicing X = dataset.iloc[:, 1:2].values . Bagaimana kita tahu X sudah menjadi matrix? Bisa dilihat kolom size di spyder variabel X adalah (10,1). Artinya X adalah matrix 10\u00d71 (10 baris dan 1 kolom). Line 9 menentukan variabel dependen y. Penting, usahakan variabel dependen adalah vector. Vektor ( vector ) adalah matriks yang hanya terdiri dari 1 kolom, atau matriks 1 baris. Cara membuatnya menjadi vektor adalah jangan lakukan slicing pada bagian kolomnya. Pada bagian size variabel y di spyder adalah (10,) yang artinya ia adalah matrix 1 baris. # Fitting Linear Regression ke dataset from`` sklearn.linear_model ``import`` LinearRegression lin_reg ``=`` LinearRegression() lin_reg.fit(X, y) Line 12 mengimpor class LinearRegression (untuk membuat model regresi sederhana) Line 13 mempersiapkan objek lin_reg sebagai model regresi sederhana Line 14 membuat model regresi sederhana (Kali ini tanpa membagi dataset ke dalam test dan train set, karena datasetnya terlalu kecil (biasanya train set minimal butuh 10 baris, dan kali ini tidak cukup data untuk dimasukkan ke test set). Walau demikian, model yang jadi nanti akan merupakan bagian dari train set, dan dataset baru yang diterima (pengujian train set) akan menjadi test set-nya). # Fitting Polynomial Regression ke dataset from`` sklearn.preprocessing ``import`` PolynomialFeatures poly_reg ``=`` PolynomialFeatures(degree ``=`` ``2``) ``## nantinya degree diganti menjadi 4 X_poly ``=`` poly_reg.fit_transform(X) lin_reg_2 ``=`` LinearRegression() lin_reg_2.fit(X_poly, y) Line 17 mengimpor PolynomialFeatures dari library sklearn.preprocessing untuk membuat model polinomial. Untuk mengetahui parameter apa saja yang diperlukan, cukup arahkan kursor pada PolynomialFeatures, lalu klik CTRL+i. Line 18 mempersiapkan objek poly_reg sebagai transformasi matriks X menjadi matriks X pangkat 2, pangkat 3 hingga pangkat n. Jadi nantinya kita memiliki beberapa tambahan variabel independen sebanyak n. Parameter default untuk PolynomialFeatures adalah degrees=2. Line 19 menyiapkan objek X_poly sebagai hasil fit_transform (proses fit dan transform dilakukan sekaligus) dari variabel X. Mari kita bandingkan antara X dengan X_poly. Line 20 menyiapkan objek lin_reg_2 sebagai model regresi polinomial. Line 21 membuat model regresi polinomial dengan parameter variabel independen adalah X_poly, dan variabel dependennya adalah y. # Visualisasi hasil regresi sederhana plt.scatter(X, y, color ``=`` ``'red'``) plt.plot(X, lin_reg.predict(X), color ``=`` ``'blue'``) plt.title(``'Sesuai atau tidak (Linear Regression)'``) plt.xlabel(``'Level posisi'``) plt.ylabel(``'Gaji'``) plt.show() Line 24 sampai line 29 adalah perintah untuk visualisasi hasil model regresi sederhana kita. Ingat untuk visualisasi, perintah dari line 24-29 harus dieksekusi bersamaan. Visualisasinya akan nampak sebagai berikut : # Visualisasi hasil regresi polynomial plt.scatter(X, y, color ``=`` ``'red'``) plt.plot(X, lin_reg_2.predict(X_poly), color ``=`` ``'blue'``) plt.title(``'Sesuai atau tidak (Polynomial Regression)'``) plt.xlabel(``'Level posisi'``) plt.ylabel(``'Gaji'``) plt.show() Line 32 sampai line 37 adalah perintah untuk visualisasi hasil model regresi polinomial. Pelru diingat sumbu y nya adalah lin_reg_2.predict(X_poly) . Hasilnya akan tampak sebagai berikut : Bisa dilihat dengan menggunakan fungsi polinomial hasilnya cukup baik. Namun tetap saja masih kurang cukup fit , di mana masih ada jarak antara model dengan data. Solusinya adalah pada line 18 kita ubah degree nya dari 2 menjadi 4. Eksekusi line 18 sampai line 21. Kemudian eksekusi line 32 sampai line 37. Maka visualisasi yang baru akan tampak sebagai berikut : # Memprediksi hasil dengan regresi sederhana lin_reg.predict(``6.5``) Line 40 adalah perintah untuk melihat dengan model regresi sederhana yang sudah dibuat, berapa gaji yang layak untuk tingkat level 6.5? Maka cukup ganti parameter X di lin_reg.predict(X) dengan angka 6.5. Jika dieksekusi, hasilnya adalah 330378.78 dollar/tahun. Tentunya prediksi dari regresi sederhana terlalu tinggi (terlihat juga di plot visualisasinya). Kita tidak menginginkan gaji yang terlalu tinggi yang merupakan hasil dari model regresi sederhana yang buruk kali ini. # Memprediksi hasil dengan regresi polynomial lin_reg_2.predict(poly_reg.fit_transform(``6.5``)) Line 43 adalah perintah untuk melihat prediksi gaji dengan model regresi polinomial. Perlu diperhatikan bahwa parameter X diganti dengan poly_reg.fit_transform(6.5) dan bukan X_poly. Karena kita ingin mengisi angka 6.5 sebagai parameter X. Sementara X_poky adalah hasil dari definisi fungsi poly_reg.fit_transform(X). Ketika dieksekusi maka hasilnya adalah 158862.45 dollar/tahun. Prediksi yang cukup baik, dengan model yang fit. MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Contoh kasus dengan penghitungan sklearn"},{"location":"tugas1komnum/","text":"Error in numerical computation \u00b6 ERROR \u00b6 \u200b Error merupakan perbedaan antara hasil penyelesaian suatu model matematik secara numeric dengan penyelesaian secara analitis. Kesalahan yang terjadi sangatlah penting, karena kesalahan dalam pemakaian algoritma pendekatan akan menyebabkan nilai kesalahan yang besar. Sehingga pendekatan metode numerik selalu membahas tingkat kesalahan dan tingkat kecepatan proses yang akan terjadi. NILAI ERROR \u00b6 \u200b Besarnya kesalahan atas suatu nilai taksiran dapat dinyatakan secara kuantitatif dan kualitatif. Besarnya kesalahan yang dinyatakan secara kuantitatif disebut Kesalahan Absolut. Besarnya kesalahan yang dinyatakan secara kualitatif disebut dengan kesalahan Relatif . Absolut error \u00b6 \u200b Kesalahan absolut suatu kuantitas adalah nilai absolut dari selisih antara nilai sebenarnya X dan nilai perkiraan x. Ini dilambangkan dengan $$ Ea = |X - x| $$ kesalahan Relatif \u00b6 \u200b Relative error biasa disebut sebagai kesalahan relatif dari suatu kuantitas adalah rasio kesalahan absolutnya terhadap nilai sebenarnya. Ini dilambangkan dengan Er. $$ Er = |Xt - Xa / Xt| $$ PENYEBAB TERJADINYA ERROR \u00b6 Dibedakan dalam beberapa macam : 1.Round-off-errors \u00b6 \u200b Perhitungan dengan metode numerik hampir selalu menggunakan bilanganriil.Masalah timbul bila komputasi numerik dikerjakan oleh mesin (dalam hal ini komputer) karena semua bilangan riil tidak dapat disajikan secara tepat di dalamkomputer. Keterbatasan komputer dalam menyajikan bilangan riil menghasilkangalat yang disebut galat pembulatan . Sebagai contoh 1/6 = 0.166666666\u2026 tidak dapat dinyatakan secara tepat oleh komputer karena digit 6 panjangnya tidak terbatas. Komputer hanya mampu merepresentasikan sejumlah digit (atau bit dalam sistem biner) saja. 2.Truncation errors \u00b6 Kesalahan pemotongan terjadi ketika suatu rumus komputasi disederhanakan dengan cara membuang suku yang berderajat tinggi. 3.Inherent error \u00b6 Terjadi akibat kekeliruan dalam menyalin data, salah membaca skala atau kesalahan karena kurangnya pengertian mengenai hukum-hukum fisik dari data yang diukur. Kesalahan ini sering terjadi karena faktor human error DEFINISI MACLAURIN \u00b6 \u200b Suatu fungsi f(x) yang memiliki turunan , , , dan seterusnya yang kontinyu dalam interval dengan maka untuk disekitar yaitu , dapat diekspansi kedalam Deret TaylorDefinisi. Berikut algoritma dari maclaurin Dengan algoritma diatas kita dapat menyerderhanakannya sebagai berikut: berikut contoh implementai dari maclaurin f(x)= e 2x $$ f(x)\u22481+2x \\displaystyle+\\frac{{{{f}^{{\\text{}}}{\\left({2x^2}\\right)}}}}{{{3}!}} \\displaystyle+\\frac{{{{f}^{{\\text{}}}{\\left({2x^3}\\right)}}}}{{{3}!}} \\displaystyle+\\ldots+\u2026 $$ sekarang kita masukan misal x=0 $$ f(0)\u22481+2(0) \\displaystyle+\\frac{{{{}^{{\\text{}}}{\\left({2(0)^2}\\right)}}}}{{{3}!}} \\displaystyle+\\frac{{{{}^{{\\text{}}}{\\left({2(0)^3}\\right)}}}}{{{3}!}} \\displaystyle+\\ldots+\u2026 $$ jadi ketika x =0 maka hasil akan tetap 1 mekipun banyak suku dan literasi Listing Program \u00b6 membuat program supaya dapaat mengekspansi bilangan e^3x dengan nilai x=4 hingga nilai menjadi kurang dari 0,001 bisa dengan listing program sebagai berikut. import math coba = 1 a = 0 b = 1 x = int ( input ( \"masukkan x = \" )) while coba > 0.001 : f_x = 0 f_y = 0 for i in range ( a ): f_x += ( 2 ** i ) * x ** i / math . factorial ( i ) for j in range ( b ): f_y += ( 2 ** j ) * x ** j / math . factorial ( j ) print ( \"suku ke \" , a , \"=\" , f_x ) print ( \"suku ke \" , b , \"=\" , f_y ) coba = f_y - f_x a += 1 b += 1 print ( \"selisih sukunya = \" , coba ) output: masukkan x = 1 suku ke 0 = 0 suku ke 1 = 1.0 selisih sukunya = 1.0 suku ke 1 = 1.0 suku ke 2 = 3.0 selisih sukunya = 2.0 suku ke 2 = 3.0 suku ke 3 = 5.0 selisih sukunya = 2.0 suku ke 3 = 5.0 suku ke 4 = 6.333333333333333 selisih sukunya = 1.333333333333333 suku ke 4 = 6.333333333333333 suku ke 5 = 7.0 selisih sukunya = 0.666666666666667 suku ke 5 = 7.0 suku ke 6 = 7.266666666666667 selisih sukunya = 0.2666666666666666 suku ke 6 = 7.266666666666667 suku ke 7 = 7.355555555555555 selisih sukunya = 0.08888888888888857 suku ke 7 = 7.355555555555555 suku ke 8 = 7.3809523809523805 selisih sukunya = 0.025396825396825307 suku ke 8 = 7.3809523809523805 suku ke 9 = 7.387301587301587 selisih sukunya = 0.006349206349206327 suku ke 9 = 7.387301587301587 suku ke 10 = 7.3887125220458545 selisih sukunya = 0.0014109347442676778 suku ke 10 = 7.3887125220458545 suku ke 11 = 7.388994708994708 selisih sukunya = 0.0002821869488531803 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$'],['$','$']]} });","title":"Error in numerical computation"},{"location":"tugas1komnum/#error-in-numerical-computation","text":"","title":"Error in numerical computation"},{"location":"tugas1komnum/#error","text":"\u200b Error merupakan perbedaan antara hasil penyelesaian suatu model matematik secara numeric dengan penyelesaian secara analitis. Kesalahan yang terjadi sangatlah penting, karena kesalahan dalam pemakaian algoritma pendekatan akan menyebabkan nilai kesalahan yang besar. Sehingga pendekatan metode numerik selalu membahas tingkat kesalahan dan tingkat kecepatan proses yang akan terjadi.","title":"ERROR"},{"location":"tugas1komnum/#nilai-error","text":"\u200b Besarnya kesalahan atas suatu nilai taksiran dapat dinyatakan secara kuantitatif dan kualitatif. Besarnya kesalahan yang dinyatakan secara kuantitatif disebut Kesalahan Absolut. Besarnya kesalahan yang dinyatakan secara kualitatif disebut dengan kesalahan Relatif .","title":"NILAI ERROR"},{"location":"tugas1komnum/#absolut-error","text":"\u200b Kesalahan absolut suatu kuantitas adalah nilai absolut dari selisih antara nilai sebenarnya X dan nilai perkiraan x. Ini dilambangkan dengan $$ Ea = |X - x| $$","title":"Absolut error"},{"location":"tugas1komnum/#kesalahan-relatif","text":"\u200b Relative error biasa disebut sebagai kesalahan relatif dari suatu kuantitas adalah rasio kesalahan absolutnya terhadap nilai sebenarnya. Ini dilambangkan dengan Er. $$ Er = |Xt - Xa / Xt| $$","title":"kesalahan Relatif"},{"location":"tugas1komnum/#penyebab-terjadinya-error","text":"Dibedakan dalam beberapa macam :","title":"PENYEBAB TERJADINYA ERROR"},{"location":"tugas1komnum/#1round-off-errors","text":"\u200b Perhitungan dengan metode numerik hampir selalu menggunakan bilanganriil.Masalah timbul bila komputasi numerik dikerjakan oleh mesin (dalam hal ini komputer) karena semua bilangan riil tidak dapat disajikan secara tepat di dalamkomputer. Keterbatasan komputer dalam menyajikan bilangan riil menghasilkangalat yang disebut galat pembulatan . Sebagai contoh 1/6 = 0.166666666\u2026 tidak dapat dinyatakan secara tepat oleh komputer karena digit 6 panjangnya tidak terbatas. Komputer hanya mampu merepresentasikan sejumlah digit (atau bit dalam sistem biner) saja.","title":"1.Round-off-errors"},{"location":"tugas1komnum/#2truncation-errors","text":"Kesalahan pemotongan terjadi ketika suatu rumus komputasi disederhanakan dengan cara membuang suku yang berderajat tinggi.","title":"2.Truncation errors"},{"location":"tugas1komnum/#3inherent-error","text":"Terjadi akibat kekeliruan dalam menyalin data, salah membaca skala atau kesalahan karena kurangnya pengertian mengenai hukum-hukum fisik dari data yang diukur. Kesalahan ini sering terjadi karena faktor human error","title":"3.Inherent error"},{"location":"tugas1komnum/#definisi-maclaurin","text":"\u200b Suatu fungsi f(x) yang memiliki turunan , , , dan seterusnya yang kontinyu dalam interval dengan maka untuk disekitar yaitu , dapat diekspansi kedalam Deret TaylorDefinisi. Berikut algoritma dari maclaurin Dengan algoritma diatas kita dapat menyerderhanakannya sebagai berikut: berikut contoh implementai dari maclaurin f(x)= e 2x $$ f(x)\u22481+2x \\displaystyle+\\frac{{{{f}^{{\\text{}}}{\\left({2x^2}\\right)}}}}{{{3}!}} \\displaystyle+\\frac{{{{f}^{{\\text{}}}{\\left({2x^3}\\right)}}}}{{{3}!}} \\displaystyle+\\ldots+\u2026 $$ sekarang kita masukan misal x=0 $$ f(0)\u22481+2(0) \\displaystyle+\\frac{{{{}^{{\\text{}}}{\\left({2(0)^2}\\right)}}}}{{{3}!}} \\displaystyle+\\frac{{{{}^{{\\text{}}}{\\left({2(0)^3}\\right)}}}}{{{3}!}} \\displaystyle+\\ldots+\u2026 $$ jadi ketika x =0 maka hasil akan tetap 1 mekipun banyak suku dan literasi","title":"DEFINISI MACLAURIN"},{"location":"tugas1komnum/#listing-program","text":"membuat program supaya dapaat mengekspansi bilangan e^3x dengan nilai x=4 hingga nilai menjadi kurang dari 0,001 bisa dengan listing program sebagai berikut. import math coba = 1 a = 0 b = 1 x = int ( input ( \"masukkan x = \" )) while coba > 0.001 : f_x = 0 f_y = 0 for i in range ( a ): f_x += ( 2 ** i ) * x ** i / math . factorial ( i ) for j in range ( b ): f_y += ( 2 ** j ) * x ** j / math . factorial ( j ) print ( \"suku ke \" , a , \"=\" , f_x ) print ( \"suku ke \" , b , \"=\" , f_y ) coba = f_y - f_x a += 1 b += 1 print ( \"selisih sukunya = \" , coba ) output: masukkan x = 1 suku ke 0 = 0 suku ke 1 = 1.0 selisih sukunya = 1.0 suku ke 1 = 1.0 suku ke 2 = 3.0 selisih sukunya = 2.0 suku ke 2 = 3.0 suku ke 3 = 5.0 selisih sukunya = 2.0 suku ke 3 = 5.0 suku ke 4 = 6.333333333333333 selisih sukunya = 1.333333333333333 suku ke 4 = 6.333333333333333 suku ke 5 = 7.0 selisih sukunya = 0.666666666666667 suku ke 5 = 7.0 suku ke 6 = 7.266666666666667 selisih sukunya = 0.2666666666666666 suku ke 6 = 7.266666666666667 suku ke 7 = 7.355555555555555 selisih sukunya = 0.08888888888888857 suku ke 7 = 7.355555555555555 suku ke 8 = 7.3809523809523805 selisih sukunya = 0.025396825396825307 suku ke 8 = 7.3809523809523805 suku ke 9 = 7.387301587301587 selisih sukunya = 0.006349206349206327 suku ke 9 = 7.387301587301587 suku ke 10 = 7.3887125220458545 selisih sukunya = 0.0014109347442676778 suku ke 10 = 7.3887125220458545 suku ke 11 = 7.388994708994708 selisih sukunya = 0.0002821869488531803 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$'],['$','$']]} });","title":"Listing Program"}]}